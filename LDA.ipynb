{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA 统计模型对新闻进行主题分析\n",
    "\n",
    "在LDA模型中，一篇文档生成的方式如下：\n",
    "1. 从狄利克雷分布中取样生成文档 i 的主题分布\n",
    "2. 从主题的多项式分布中取样生成文档i第 j 个词的主题\n",
    "3. 从狄利克雷分布中取样生成主题对应的词语分布\n",
    "4. 从词语的多项式分布中采样最终生成词语"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新闻爬取xlxs文件目录：\n",
    "NEWS_PATH = '../news/'\n",
    "#语料库文件路径：\n",
    "DATA_PATH = '../data/news_words.txt'\n",
    "#标记数据路径\n",
    "LABEL_PATH = '../labels/'\n",
    "\n",
    "#coding=utf-8  \n",
    "import codecs  \n",
    "from gensim import corpora  \n",
    "from gensim.models import LdaModel  \n",
    "from gensim.corpora import Dictionary  \n",
    "\n",
    "\n",
    "import jieba\n",
    "import csv\n",
    "import numpy as np  \n",
    "import os  \n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "   \n",
    "import time    \n",
    "from sklearn import metrics    \n",
    "import pickle as pickle    \n",
    "import pandas as pd  \n",
    "  \n",
    "import random\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 读取语料库，载入字典\n",
    "\n",
    "# print(\"Start reading corpus file...\")\n",
    "# start_time = time.time()    \n",
    "# fr=open(DATA_PATH,'r')  \n",
    "# train=[]  \n",
    "# for line in fr.readlines():  \n",
    "#     line=line.split(' ')  \n",
    "#     train.append(line)  \n",
    "# print(len(train))\n",
    "# print(' '.join(train[2]))\n",
    "\n",
    "# dictionary = corpora.Dictionary(train)  \n",
    "# print('Composing dictionary took %fs!' % (time.time() - start_time)) \n",
    "# start_time = time.time()\n",
    "# corpus = [ dictionary.doc2bow(text) for text in train ]  \n",
    "# print('Loading corpus took %fs!' % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading corpus file...\n",
      "Composing dictionary took 249.582738s!\n",
      "Loading corpus took 0.000119s!\n"
     ]
    }
   ],
   "source": [
    "# 遍历语料库文件，逐步增加dictionary\n",
    "print(\"Start reading corpus file...\")\n",
    "start_time = time.time()    \n",
    "fr=open(DATA_PATH,'r')  \n",
    "train=[]  \n",
    "dictionary = corpora.Dictionary()\n",
    "for line in fr.readlines():  \n",
    "    line=line.split(' ')  \n",
    "    dictionary.add_documents([line]) \n",
    "#dictionary.doc2bow([\"军工\",\"金融\"])\n",
    "print('Composing dictionary took %fs!' % (time.time() - start_time)) \n",
    "start_time = time.time()\n",
    "corpus = [ dictionary.doc2bow(text) for text in train ]  \n",
    "print('Loading corpus took %fs!' % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA training took 93.062531s!\n"
     ]
    }
   ],
   "source": [
    "#训练LDA模型\n",
    "start_time = time.time()    \n",
    "lda = LdaModel(corpus=corpus, id2word=dictionary, num_topics=200)  \n",
    "lda.save('../model/lda.model')\n",
    "print('LDA training took %fs!' % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel.load('../model/lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "20\n",
      "(160, '0.000*\"90.377\" + 0.000*\"徐书楠\" + 0.000*\"对运来\" + 0.000*\"1722.2\" + 0.000*\"0.07001458\" + 0.000*\"王宝敏\" + 0.000*\"城中心\" + 0.000*\"7825.2525\" + 0.000*\"ICN\" + 0.000*\"以发\"')\n",
      "(65, '0.000*\"125403.97\" + 0.000*\"岑赛\" + 0.000*\"衔接\" + 0.000*\"4030\" + 0.000*\"40.38\" + 0.000*\"19.5010\" + 0.000*\"同退\" + 0.000*\"摆在首位\" + 0.000*\"顶头上司\" + 0.000*\"横移类\"')\n",
      "(82, '0.000*\"文著\" + 0.000*\"止马营\" + 0.000*\"20.74300059\" + 0.000*\"骗税\" + 0.000*\"70.719\" + 0.000*\"4984.27\" + 0.000*\"863.71002153\" + 0.000*\"37000682\" + 0.000*\"259.389\" + 0.000*\"科佩茨\"')\n",
      "(197, '0.000*\"455.395\" + 0.000*\"1440.1\" + 0.000*\"叶贤林\" + 0.000*\"247.544\" + 0.000*\"20160808653542465\" + 0.000*\"15413.74\" + 0.000*\"立户\" + 0.000*\"96257.6323421\" + 0.000*\"OHSASI8001\" + 0.000*\"094533.88\"')\n",
      "(92, '0.000*\"5.8900338\" + 0.000*\"InstituteofInternationalFinance\" + 0.000*\"35.0025\" + 0.000*\"9033.597\" + 0.000*\"8661.651\" + 0.000*\"23.12000831\" + 0.000*\"317.9256\" + 0.000*\"使内\" + 0.000*\"103.11300406\" + 0.000*\"德敖东\"')\n",
      "(130, '0.000*\"82.1645\" + 0.000*\"价来\" + 0.000*\"3465.20\" + 0.000*\"罗地亚\" + 0.000*\"24373.18\" + 0.000*\"Neil\" + 0.000*\"紧普五\" + 0.000*\"ChinaDepositoryReceipt\" + 0.000*\"8.4949\" + 0.000*\"孟萌\"')\n",
      "(9, '0.000*\"20160114585291426\" + 0.000*\"54603817\" + 0.000*\"13.2155\" + 0.000*\"4166.63\" + 0.000*\"9063.48\" + 0.000*\"759.98300420\" + 0.000*\"270022.009958728\" + 0.000*\"401.38603505\" + 0.000*\"InternetinCar\" + 0.000*\"樟子\"')\n",
      "(147, '0.000*\"互镜\" + 0.000*\"16892.91\" + 0.000*\"25.26002180\" + 0.000*\"远场\" + 0.000*\"抗刮\" + 0.000*\"6214.5114\" + 0.000*\"062.010\" + 0.000*\"PE32.3\" + 0.000*\"5537.45281\" + 0.000*\"15.5600895\"')\n",
      "(129, '0.000*\"壮心\" + 0.000*\"08600026\" + 0.000*\"39.12\" + 0.000*\"购物广场\" + 0.000*\"1109.17\" + 0.000*\"亿北\" + 0.000*\"钱仁清\" + 0.000*\"10.742017\" + 0.000*\"6773.947\" + 0.000*\"云自研\"')\n",
      "(85, '0.000*\"8317.46\" + 0.000*\"0.98006581\" + 0.000*\"以入\" + 0.000*\"星亚\" + 0.000*\"30243.2313\" + 0.000*\"九鼎投资\" + 0.000*\"2517.00\" + 0.000*\"760.91000545\" + 0.000*\"U10971P31DT20160103160512\" + 0.000*\"4895.62\"')\n",
      "(90, '0.000*\"万顶格\" + 0.000*\"刘彦春则\" + 0.000*\"929.489\" + 0.000*\"400.06300722\" + 0.000*\"910pc\" + 0.000*\"忌食\" + 0.000*\"011.13002898\" + 0.000*\"果瑞\" + 0.000*\"高管开\" + 0.000*\"12338.0067\"')\n",
      "(31, '0.000*\"中含\" + 0.000*\"728.36\" + 0.000*\"2825.22\" + 0.000*\"2648.93\" + 0.000*\"87.645\" + 0.000*\"126882\" + 0.000*\"大气层\" + 0.000*\"5419.4519\" + 0.000*\"600267\" + 0.000*\"5776.77\"')\n",
      "(86, '0.000*\"ArnaudMasset\" + 0.000*\"8008.338\" + 0.000*\"第十四位\" + 0.000*\"研率\" + 0.000*\"2892.534\" + 0.000*\"伪到\" + 0.000*\"探至\" + 0.000*\"13499802.0099171\" + 0.000*\"672.440\" + 0.000*\"9.227\"')\n",
      "(178, '0.000*\"81441.187\" + 0.000*\"11.60600589\" + 0.000*\"957.50\" + 0.000*\"李川摄\" + 0.000*\"4.92107\" + 0.000*\"461.07\" + 0.000*\"165.515\" + 0.000*\"002602\" + 0.000*\"594.56300040\" + 0.000*\"经沪港通\"')\n",
      "(108, '0.000*\"01101.54300151\" + 0.000*\"LadyLoretta\" + 0.000*\"工频\" + 0.000*\"日语\" + 0.000*\"92.3413\" + 0.000*\"YukioIshizuki\" + 0.000*\"MinisterMohammadRezaNematzadeh\" + 0.000*\"191.36600827\" + 0.000*\"甜酒\" + 0.000*\"969.71\"')\n",
      "(22, '0.000*\"653234\" + 0.000*\"008750.00\" + 0.000*\"1211.1811\" + 0.000*\"付汇\" + 0.000*\"28.649111\" + 0.000*\"27.2915\" + 0.000*\"96430000917\" + 0.000*\"66.1711\" + 0.000*\"69.4530\" + 0.000*\"3659.56\"')\n",
      "(189, '0.000*\"滨和程\" + 0.000*\"97134.2513\" + 0.000*\"32965.72\" + 0.000*\"27601998\" + 0.000*\"2361.59\" + 0.000*\"卓利伟\" + 0.000*\"0.0288000063\" + 0.000*\"3300679\" + 0.000*\"55.683\" + 0.000*\"1386.521\"')\n",
      "(96, '0.000*\"九县\" + 0.000*\"齐格弗\" + 0.000*\"1201.34\" + 0.000*\"11.24002698\" + 0.000*\"5277.4\" + 0.000*\"10.77002402\" + 0.000*\"6574065\" + 0.000*\"流速\" + 0.000*\"3.16002439\" + 0.000*\"星沙\"')\n",
      "(71, '0.000*\"KYMGJ\" + 0.000*\"54600917\" + 0.000*\"231.11600270\" + 0.000*\"9152899\" + 0.000*\"鲍德里\" + 0.000*\"李森柏\" + 0.000*\"65.976\" + 0.000*\"互惠待遇\" + 0.000*\"2051.56\" + 0.000*\"6569.92\"')\n",
      "(87, '0.000*\"66.3119\" + 0.000*\"580.13600614\" + 0.000*\"G3015\" + 0.000*\"血迹\" + 0.000*\"11.74002517\" + 0.000*\"了旭辉\" + 0.000*\"22.8830\" + 0.000*\"642.05603958\" + 0.000*\"0.290019423\" + 0.000*\"FIRSTEXINC\"')\n",
      "第一主题\n",
      "0.000*\"389.679\" + 0.000*\"同环\" + 0.000*\"7.7402338\" + 0.000*\"887.821\" + 0.000*\"600.78000902\" + 0.000*\"000.2018\" + 0.000*\"22653.48\" + 0.000*\"0.5332\" + 0.000*\"0.58002414\" + 0.000*\"668.668\"\n"
     ]
    }
   ],
   "source": [
    "# 打印LDA模型相关结果\n",
    "topic_list=lda.print_topics(20)  \n",
    "print(type(lda.print_topics(20)))\n",
    "print(len(lda.print_topics(20)))\n",
    "  \n",
    "for topic in topic_list:  \n",
    "    print(topic) \n",
    "print(\"第一主题\")\n",
    "print(lda.print_topic(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 标记数据导入与分类训练\n",
    "\n",
    "标记数据来源： 第一财经新闻\n",
    "标记格式：新闻标题/正文/关键词\n",
    "利用LDA的模型，将标题+正文作为字符串，对标记数据进行向量化，标签为关键词。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.143 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 加载停用词，输入停用词文件，输出停用词list\n",
    "def stopwordslist(filepath):  \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]  \n",
    "    return stopwords\n",
    "\n",
    "temp_stop_list = ['\\u3000','\\xa0','\\t']\n",
    "stop_words = stopwordslist(\"../utils/stopwords.txt\") + temp_stop_list\n",
    "# 为结巴分词词库加载股票名词汇\n",
    "jieba.load_userdict('../data/user_dict.txt')\n",
    "\n",
    " \n",
    "## jieba分词：输入string & 停用词文件，输出分词结果list\n",
    "def jieba_split(content):\n",
    "    '''\n",
    "    content: 输入文本（string）\n",
    "    stop_path: 停用词字典文件路径（string）\n",
    "    返回：list，jieba分词结果\n",
    "    '''\n",
    "    str_content = str(content).replace('\\t', '').replace('\\n', '').replace(' ','')\n",
    "    str_words = ','.join(jieba.cut_for_search(str_content)).split(\",\")\n",
    "    ret_list = []\n",
    "    for word in str_words:\n",
    "        if word not in stop_words:\n",
    "            if word[-1] != '%':\n",
    "                ret_list.append(word)\n",
    "    return ret_list\n",
    "\n",
    "\n",
    "\n",
    "# turn lda result into list\n",
    "def lda2list(lda,topic_n):\n",
    "    lda_dict = dict(lda)\n",
    "    lda_list = [0] * topic_n\n",
    "    for i in range(topic_n):\n",
    "        lda_list[i] = lda_dict.get(i,0)\n",
    "    return lda_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['第一财经板块对应新闻-0423-2.csv', '第一财经板块对应新闻-0423-1.csv', '第一财经板块对应新闻-0419-2.csv', '第一财经板块对应新闻-0413.csv', '第一财经板块对应新闻.csv', '第一财经板块对应新闻-0419-1.csv', '第一财经板块对应新闻-0416.csv']\n",
      "../labels/第一财经板块对应新闻-0423-2.csv\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dictionary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-e7c888546533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(row.content)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mnews_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjieba_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mnews_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_word\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m#文档转换成bow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mnews_lda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda2list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnews_bow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m201\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#得到lda向量\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mnews_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnews_lda\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dictionary' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 对于label文件夹下的CSV，读取后将标题+正文转换为分词结果，再转换为LDA结果，形成200维向量\n",
    "# 对于新闻的标签，直接保留其表格中的“关键词”地段\n",
    "\n",
    "#df_label = pd.DataFrame(columns=['words','label'])  \n",
    "news_vec = []\n",
    "news_labels = []\n",
    "\n",
    "\n",
    "files = os.listdir(LABEL_PATH)\n",
    "print(files)\n",
    "for fname in files:\n",
    "    fpath = LABEL_PATH + fname\n",
    "    if 'csv' in fpath:\n",
    "        print(fpath)\n",
    "        file_data = pd.read_csv(fpath)\n",
    "        file_data.rename(columns={'标题':'title', '正文':'content','正文1':'content',\"字段1_文本\":\"title\",\"关键词\":\"plate\"}, inplace = True)\n",
    "        for index, row in file_data.iterrows():\n",
    "        #print(row.content)\n",
    "            news_word = jieba_split(str(row.title) + str(row.content))\n",
    "            news_bow = dictionary.doc2bow(news_word)      #文档转换成bow  \n",
    "            news_lda = lda2list(lda[news_bow],201) #得到lda向量\n",
    "            news_vec.append(news_lda)\n",
    "            news_labels.append(row.plate)\n",
    "    print(\"done\")\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55793\n"
     ]
    }
   ],
   "source": [
    "print(len(news_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "55793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"'' \\npartial_fit说明：增量的训练一批样本 \\n这种方法被称为连续几次在不同的数据集，从而实现核心和在线学习，这是特别有用的，当数据集很大的时候，不适合在内存中运算 \\n该方法具有一定的性能和数值稳定性的开销，因此最好是作用在尽可能大的数据块（只要符合内存的预算开销） \\n\""
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#高斯朴素贝叶斯  \n",
    "X=np.array([np.array(xi) for xi in news_vec])\n",
    "Y = np.array(news_labels)\n",
    "print(type(X[0]))\n",
    "print(type(Y))\n",
    "print(len(X))\n",
    "#X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])  \n",
    "#Y = np.array([1, 1, 1, 2, 2, 2])  \n",
    "clf = GaussianNB().fit(X, Y)  \n",
    "#print(clf.predict([[-0.8,-1]]))\n",
    "  \n",
    "''''' \n",
    "partial_fit说明：增量的训练一批样本 \n",
    "这种方法被称为连续几次在不同的数据集，从而实现核心和在线学习，这是特别有用的，当数据集很大的时候，不适合在内存中运算 \n",
    "该方法具有一定的性能和数值稳定性的开销，因此最好是作用在尽可能大的数据块（只要符合内存的预算开销） \n",
    "'''  \n",
    "#clf_pf = GaussianNB().partial_fit(X, Y, np.unique(Y))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# sklearn中几个基本的分类其的训练函数\n",
    "\n",
    "# Multinomial Naive Bayes Classifier    \n",
    "def naive_bayes_classifier(train_x, train_y):    \n",
    "    from sklearn.naive_bayes import MultinomialNB    \n",
    "    model = MultinomialNB(alpha=0.01)    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "    \n",
    "    \n",
    "# KNN Classifier    \n",
    "def knn_classifier(train_x, train_y):    \n",
    "    from sklearn.neighbors import KNeighborsClassifier    \n",
    "    model = KNeighborsClassifier()    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "    \n",
    "    \n",
    "# Logistic Regression Classifier    \n",
    "def logistic_regression_classifier(train_x, train_y):    \n",
    "    from sklearn.linear_model import LogisticRegression    \n",
    "    model = LogisticRegression(penalty='l2')    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "    \n",
    "    \n",
    "# Random Forest Classifier    \n",
    "def random_forest_classifier(train_x, train_y):    \n",
    "    from sklearn.ensemble import RandomForestClassifier    \n",
    "    model = RandomForestClassifier(n_estimators=8)    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "    \n",
    "    \n",
    "# Decision Tree Classifier    \n",
    "def decision_tree_classifier(train_x, train_y):    \n",
    "    from sklearn import tree    \n",
    "    model = tree.DecisionTreeClassifier()    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "    \n",
    "    \n",
    "# GBDT(Gradient Boosting Decision Tree) Classifier    \n",
    "def gradient_boosting_classifier(train_x, train_y):    \n",
    "    from sklearn.ensemble import GradientBoostingClassifier    \n",
    "    model = GradientBoostingClassifier(n_estimators=200)    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "    \n",
    "    \n",
    "# SVM Classifier    \n",
    "def svm_classifier(train_x, train_y):    \n",
    "    from sklearn.svm import SVC    \n",
    "    model = SVC(kernel='rbf', probability=True)    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "    \n",
    "# SVM Classifier using cross validation    \n",
    "def svm_cross_validation(train_x, train_y):    \n",
    "    from sklearn.grid_search import GridSearchCV    \n",
    "    from sklearn.svm import SVC    \n",
    "    model = SVC(kernel='rbf', probability=True)    \n",
    "    param_grid = {'C': [1e-3, 1e-2, 1e-1, 1, 10, 100, 1000], 'gamma': [0.001, 0.0001]}    \n",
    "    grid_search = GridSearchCV(model, param_grid, n_jobs = 1, verbose=1)    \n",
    "    grid_search.fit(train_x, train_y)    \n",
    "    best_parameters = grid_search.best_estimator_.get_params()    \n",
    "    for para, val in list(best_parameters.items()):    \n",
    "        print(para, val)    \n",
    "    model = SVC(kernel='rbf', C=best_parameters['C'], gamma=best_parameters['gamma'], probability=True)    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "    \n",
    "def read_data(data_file):    \n",
    "    data = pd.read_csv(data_file)  \n",
    "    train = data[:int(len(data)*0.9)]  \n",
    "    test = data[int(len(data)*0.9):]  \n",
    "    train_y = train.label  \n",
    "    train_x = train.drop('label', axis=1)  \n",
    "    test_y = test.label  \n",
    "    test_x = test.drop('label', axis=1)  \n",
    "    return train_x, train_y, test_x, test_y  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将读取label新闻转换的数据存成numpy array X， Y\n",
    "# 利用X Y 进行分类训练，并查看关键词预测结果\n",
    "\n",
    "X = np.array([np.array(xi) for xi in news_vec])\n",
    "Y = np.array(news_labels)\n",
    "\n",
    "test_classifiers = ['NB', 'KNN', 'LR', 'RF', 'DT', 'SVM','SVMCV', 'GBDT']    \n",
    "classifiers = {'NB':naive_bayes_classifier,     \n",
    "              'KNN':knn_classifier,    \n",
    "               'LR':logistic_regression_classifier,    \n",
    "               'RF':random_forest_classifier,    \n",
    "               'DT':decision_tree_classifier,    \n",
    "              'SVM':svm_classifier,    \n",
    "            'SVMCV':svm_cross_validation,    \n",
    "             'GBDT':gradient_boosting_classifier    \n",
    "}    \n",
    "\n",
    "def test_classifier(classifier):\n",
    "    print(\"********classifier: \",classifier,\"***********\")\n",
    "    start_time = time.time()    \n",
    "    model = classifiers[classifier](X, Y)\n",
    "    print('training took %fs!' % (time.time() - start_time)) \n",
    "    for i in range(100):\n",
    "        n = random.randrange(1,len(X))\n",
    "        # predict_proba(x)：给出带有概率值的结果。每个点在所有label的概率和为1.  \n",
    "        # predict(x)：直接给出预测结果。内部还是调用的predict_proba()，根据概率的结果看哪个类型的预测值最高就是哪个类型。  \n",
    "        print(model.predict([X[n]]))\n",
    "        print(Y[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "#lr是一个LogisticRegression模型\n",
    "joblib.dump(model, 'rf.model')\n",
    "lr = joblib.load('lr.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********classifier:  KNN ***********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 15.636353s!\n",
      "['创投']\n",
      "创投\n",
      "['互联网彩票']\n",
      "金融IC\n",
      "['军民融合']\n",
      "车联网\n",
      "['农机']\n",
      "太阳能\n",
      "['PM2.5']\n",
      "金改\n",
      "['上海自贸区']\n",
      "福建自贸区\n",
      "['举牌']\n",
      "共享单车\n",
      "['迪士尼']\n",
      "迪士尼\n",
      "['互联网医疗']\n",
      "互联网医疗\n",
      "['上海自贸区']\n",
      "上海自贸区\n",
      "['共享单车']\n",
      "太阳能\n",
      "['基因测序']\n",
      "节能环保\n",
      "['军工']\n",
      "黄金\n",
      "['京津冀一体化']\n",
      "金改\n",
      "['PM2.5']\n",
      "风电\n",
      "['广东自贸区']\n",
      "广东自贸区\n",
      "['互联网金融']\n",
      "航运\n",
      "['农机']\n",
      "钛白粉\n",
      "['基因测序']\n",
      "基因测序\n",
      "['军工']\n",
      "军工\n",
      "['区块链']\n",
      "锂电池\n",
      "['农村电商']\n",
      "车联网\n",
      "['阿里巴巴']\n",
      "阿里巴巴\n",
      "['供应链金融']\n",
      "供应链金融\n",
      "['电子信息']\n",
      "量子通信\n",
      "['基因测序']\n",
      "航运\n",
      "['电子竞技']\n",
      "白酒\n",
      "['啤酒']\n",
      "大数据\n",
      "['大飞机']\n",
      "太阳能\n",
      "['军工']\n",
      "军工\n",
      "['举牌']\n",
      "举牌\n",
      "['PM2.5']\n",
      "PM2.5\n",
      "['人脸识别']\n",
      "迪士尼\n",
      "['人工智能']\n",
      "黄金\n",
      "['阿里巴巴']\n",
      "手机游戏\n",
      "['深港通']\n",
      "深港通\n",
      "['核电']\n",
      "核电\n",
      "['互联网金融']\n",
      "互联网金融\n",
      "['阿里巴巴']\n",
      "阿里巴巴\n",
      "['人脸识别']\n",
      "互联网金融\n",
      "['工业4.0']\n",
      "工业4.0\n",
      "['电子商务']\n",
      "广东自贸区\n",
      "['锂电池']\n",
      "锂电池\n",
      "['农业现代化']\n",
      "风电\n",
      "['供应链金融']\n",
      "供应链金融\n",
      "['跨境电商']\n",
      "跨境电商\n",
      "['锂电池']\n",
      "锂电池\n",
      "['互联网+']\n",
      "生物医药\n",
      "['高端装备']\n",
      "煤化工\n",
      "['共享单车']\n",
      "区块链\n",
      "['互联网+']\n",
      "军民融合\n",
      "['上海自贸区']\n",
      "汽车电子\n",
      "['迪士尼']\n",
      "迪士尼\n",
      "['啤酒']\n",
      "啤酒\n",
      "['石墨烯']\n",
      "石墨烯\n",
      "['上海自贸区']\n",
      "上海自贸区\n",
      "['高送转']\n",
      "节能环保\n",
      "['广东自贸区']\n",
      "广东自贸区\n",
      "['OLED']\n",
      "水泥\n",
      "['PM2.5']\n",
      "PM2.5\n",
      "['高端装备']\n",
      "高端装备\n",
      "['互联网金融']\n",
      "工业4.0\n",
      "['电子信息']\n",
      "跨境电商\n",
      "['核电']\n",
      "核电\n",
      "['农业现代化']\n",
      "美丽中国\n",
      "['PM2.5']\n",
      "PM2.5\n",
      "['安防']\n",
      "大数据\n",
      "['共享单车']\n",
      "钛白粉\n",
      "['区块链']\n",
      "区块链\n",
      "['阿里巴巴']\n",
      "人工智能\n",
      "['ST板块']\n",
      "风电\n",
      "['迪士尼']\n",
      "区块链\n",
      "['工业4.0']\n",
      "工业4.0\n",
      "['啤酒']\n",
      "高铁\n",
      "['军工']\n",
      "军工\n",
      "['煤化工']\n",
      "煤化工\n",
      "['军民融合']\n",
      "军民融合\n",
      "['互联网彩票']\n",
      "互联网彩票\n",
      "['跨境电商']\n",
      "跨境电商\n",
      "['黄金']\n",
      "黄金\n",
      "['互联网金融']\n",
      "阿里巴巴\n",
      "['高送转']\n",
      "高送转\n",
      "['互联网+']\n",
      "航运\n",
      "['互联网彩票']\n",
      "融资融券\n",
      "['迪士尼']\n",
      "迪士尼\n",
      "['水泥']\n",
      "高铁\n",
      "['PM2.5']\n",
      "PM2.5\n",
      "['区块链']\n",
      "区块链\n",
      "['互联网彩票']\n",
      "超导\n",
      "['军民融合']\n",
      "军民融合\n",
      "['人脸识别']\n",
      "人脸识别\n",
      "['上海自贸区']\n",
      "上海自贸区\n",
      "['人工智能']\n",
      "大数据\n",
      "['OLED']\n",
      "汽车电子\n",
      "['融资融券']\n",
      "上海自贸区\n",
      "['太阳能']\n",
      "高端装备\n",
      "['OLED']\n",
      "OLED\n",
      "['举牌']\n",
      "节能环保\n",
      "['迪士尼']\n",
      "迪士尼\n",
      "['PM2.5']\n",
      "核电\n"
     ]
    }
   ],
   "source": [
    "test_classifier(\"KNN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********classifier:  RF ***********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training took 5.989762s!\n",
      "['高端装备']\n",
      "高端装备\n",
      "['水泥']\n",
      "水泥\n",
      "['融资融券']\n",
      "融资融券\n",
      "['白酒']\n",
      "白酒\n",
      "['创投']\n",
      "创投\n",
      "['黄金']\n",
      "黄金\n",
      "['风电']\n",
      "风电\n",
      "['生态农业']\n",
      "生态农业\n",
      "['海工装备']\n",
      "海工装备\n",
      "['航运']\n",
      "航运\n",
      "['建筑节能']\n",
      "建筑节能\n",
      "['生物质能']\n",
      "生物质能\n",
      "['量子通信']\n",
      "量子通信\n",
      "['乳业']\n",
      "黄金\n",
      "['风电']\n",
      "风电\n",
      "['能源互联网']\n",
      "能源互联网\n",
      "['乳业']\n",
      "乳业\n",
      "['电子发票']\n",
      "电子发票\n",
      "['量子通信']\n",
      "量子通信\n",
      "['军民融合']\n",
      "高校\n",
      "['集成电路']\n",
      "集成电路\n",
      "['白马股']\n",
      "白马股\n",
      "['期货概念']\n",
      "期货概念\n",
      "['冷链物流']\n",
      "冷链物流\n",
      "['阿里巴巴']\n",
      "阿里巴巴\n",
      "['水利']\n",
      "水利\n",
      "['军民融合']\n",
      "军民融合\n",
      "['农业现代化']\n",
      "农业现代化\n",
      "['阿里巴巴']\n",
      "阿里巴巴\n",
      "['食品安全']\n",
      "食品安全\n",
      "['高端装备']\n",
      "高端装备\n",
      "['基因测序']\n",
      "基因测序\n",
      "['区块链']\n",
      "区块链\n",
      "['充电桩']\n",
      "充电桩\n",
      "['太阳能']\n",
      "太阳能\n",
      "['举牌']\n",
      "举牌\n",
      "['基因测序']\n",
      "基因测序\n",
      "['农机']\n",
      "农机\n",
      "['大飞机']\n",
      "大飞机\n",
      "['共享单车']\n",
      "共享单车\n",
      "['阿里巴巴']\n",
      "阿里巴巴\n",
      "['太阳能']\n",
      "太阳能\n",
      "['航运']\n",
      "航运\n",
      "['生物医药']\n",
      "生物医药\n",
      "['工业4.0']\n",
      "工业4.0\n",
      "['电子商务']\n",
      "电子商务\n",
      "['工业4.0']\n",
      "工业4.0\n",
      "['大飞机']\n",
      "大飞机\n",
      "['核电']\n",
      "核电\n",
      "['大数据']\n",
      "大数据\n",
      "['互联网+']\n",
      "互联网+\n",
      "['创投']\n",
      "创投\n",
      "['农村电商']\n",
      "农村电商\n",
      "['创投']\n",
      "创投\n",
      "['食品安全']\n",
      "食品安全\n",
      "['农村电商']\n",
      "农村电商\n",
      "['阿里巴巴']\n",
      "阿里巴巴\n",
      "['基因测序']\n",
      "基因测序\n",
      "['美丽中国']\n",
      "美丽中国\n",
      "['高铁']\n",
      "高铁\n",
      "['水泥']\n",
      "高校\n",
      "['钛白粉']\n",
      "钛白粉\n",
      "['车联网']\n",
      "车联网\n",
      "['水泥']\n",
      "水泥\n",
      "['太阳能']\n",
      "太阳能\n",
      "['人脸识别']\n",
      "人脸识别\n",
      "['白酒']\n",
      "白酒\n",
      "['军民融合']\n",
      "军民融合\n",
      "['互联网金融']\n",
      "互联网金融\n",
      "['农机']\n",
      "农机\n",
      "['高铁']\n",
      "高铁\n",
      "['量子通信']\n",
      "量子通信\n",
      "['融资融券']\n",
      "融资融券\n",
      "['深港通']\n",
      "深港通\n",
      "['乳业']\n",
      "乳业\n",
      "['迪士尼']\n",
      "黄金\n",
      "['区块链']\n",
      "区块链\n",
      "['高送转']\n",
      "高送转\n",
      "['充电桩']\n",
      "充电桩\n",
      "['工业4.0']\n",
      "工业4.0\n",
      "['阿里巴巴']\n",
      "阿里巴巴\n",
      "['美丽中国']\n",
      "美丽中国\n",
      "['黄金']\n",
      "黄金\n",
      "['上海自贸区']\n",
      "上海自贸区\n",
      "['家用电器']\n",
      "家用电器\n",
      "['PPP概念']\n",
      "PPP概念\n",
      "['跨境电商']\n",
      "跨境电商\n",
      "['共享单车']\n",
      "共享单车\n",
      "['节能环保']\n",
      "节能环保\n",
      "['汽车电子']\n",
      "汽车电子\n",
      "['军工']\n",
      "军工\n",
      "['充电桩']\n",
      "充电桩\n",
      "['军工']\n",
      "军工\n",
      "['车联网']\n",
      "车联网\n",
      "['美丽中国']\n",
      "美丽中国\n",
      "['水泥']\n",
      "水泥\n",
      "['迪士尼']\n",
      "迪士尼\n",
      "['集成电路']\n",
      "集成电路\n",
      "['家用电器']\n",
      "跨境电商\n",
      "['钛白粉']\n",
      "乳业\n"
     ]
    }
   ],
   "source": [
    "test_classifier(\"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-063d3c5c34b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SVM\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test_classifier' is not defined"
     ]
    }
   ],
   "source": [
    "test_classifier(\"SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
