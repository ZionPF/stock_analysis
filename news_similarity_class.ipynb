{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.880 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果：\n",
      "['孙', '公司', '海航', '地产', '拟', '海南', '融创昌晟', '签订', '股权', '转让', '协议', '出售', '海航', '地产', '持有', '海岛', '物流', '股权', '转让', '价款', '约', '亿元', '海航', '地产', '拟', '出售', '持有', '海南', '高', '房地', '地产', '房地产', '开发', '有限', '公司', '有限公司', '股权', '海南', '融创昌晟', '转让', '价款', '约', '亿元', '责任', '编辑', '责任编辑', '张恒']\n",
      "load 1h news success\n",
      "['1526458569', '1526458570', '1526458571', '1526458572', '1526458573', '1526458574', '1526458575', '1526458576', '1526458577', '1526458578', '1526458579', '1526458580', '1526458581', '1526458582', '1526458583', '1526458584', '1526458585', '1526458586', '1526458587', '1526458588', '1526458589', '1526458590', '1526458591', '1526458592', '1526458593', '1526458594', '1526458595', '1526458596', '1526458597', '1526458598', '1526458599', '1526458600', '1526458601', '1526458602', '1526458603', '1526458604', '1526458605', '1526458606', '1526458607', '1526458608', '1526458609', '1526458610', '1526458611', '1526458612', '1526458613', '1526458614', '1526458615', '1526458616', '1526458617', '1526458618', '1526458619', '1526458620', '1526458621', '1526458622', '1526458623', '1526458624', '1526458625', '1526458626', '1526458627', '1526458628', '1526458629', '1526458630', '1526458631', '1526458632', '1526458633', '1526458634', '1526458635', '1526458636', '1526458637', '1526458638', '1526458639', '1526458640', '1526458641', '1526458642', '1526458643', '1526458644', '1526458645', '1526458646', '1526458647', '1526458648', '1526458649', '1526458650', '1526458651', '1526458652', '1526458653', '1526458654', '1526458655', '1526458656', '1526458657', '1526458658', '1526458659', '1526458660', '1526458661', '1526458662', '1526458663', '1526458664', '1526458665', '1526458666', '1526458667', '1526458668', '1526458669', '1526458670', '1526458671', '1526458672', '1526458673', '1526458674', '1526458675', '1526458676', '1526458677', '1526458678', '1526458679', '1526458680', '1526458681', '1526458682', '1526458683', '1526458684', '1526458685', '1526458686', '1526458687', '1526458688', '1526458689', '1526458690', '1526458691', '1526458692', '1526458693', '1526458694', '1526458695', '1526458696', '1526458697', '1526458698', '1526458699', '1526458700', '1526458701', '1526458702', '1526458703', '1526458704', '1526458705', '1526458706', '1526458707', '1526458708', '1526458709', '1526458710', '1526458711', '1526458712', '1526458713', '1526458714', '1526458715', '1526458716', '1526458717', '1526458718', '1526458719', '1526458720', '1526458721', '1526458722', '1526458723', '1526458724', '1526458725', '1526458726', '1526458727', '1526458728', '1526458729', '1526458730', '1526458731', '1526458732', '1526458733', '1526458734', '1526458735', '1526458736', '1526458737', '1526458738', '1526458739', '1526458740', '1526458741', '1526458742', '1526458743', '1526458744', '1526458745', '1526458746', '1526458747', '1526458748', '1526458749', '1526458750', '1526458751', '1526458752', '1526458753', '1526458754', '1526458755', '1526458756', '1526458757', '1526458758', '1526458759', '1526458760', '1526458761', '1526458762', '1526458763', '1526458764', '1526458765', '1526458766', '1526458767', '1526458768', '1526458769', '1526458770', '1526458771', '1526458772', '1526458773', '1526458774', '1526458775', '1526458776', '1526458777', '1526458778', '1526458779', '1526458780', '1526458781', '1526458782', '1526458783', '1526458784', '1526458785', '1526458786', '1526458787', '1526458788', '1526458789', '1526458790', '1526458791', '1526458792', '1526458793', '1526458794', '1526458795', '1526458796', '1526458797', '1526458798', '1526458799', '1526458800', '1526458801', '1526458802', '1526458803', '1526458804', '1526458805', '1526458806', '1526458807', '1526458808', '1526458809', '1526458810', '1526458811', '1526458812', '1526458813', '1526458814', '1526458815', '1526458816', '1526458817', '1526458818', '1526458819', '1526458820', '1526458821', '1526458822', '1526458823', '1526458824', '1526458825', '1526458826', '1526458827', '1526458828', '1526458829', '1526458830', '1526458831', '1526458832', '1526458833', '1526458834', '1526458835', '1526458836', '1526458837', '1526458838', '1526458839', '1526458840', '1526458841', '1526458842', '1526458843', '1526458844', '1526458845', '1526458846', '1526458847', '1526458848', '1526458849', '1526458850', '1526458851', '1526458852', '1526458853', '1526458854', '1526458855', '1526458856', '1526458857', '1526458858', '1526458859', '1526458860', '1526458861', '1526458862', '1526458863', '1526458864', '1526458865', '1526458866', '1526458867', '1526458868', '1526458869', '1526458870', '1526458871', '1526458872', '1526458873', '1526458874', '1526458875', '1526458876', '1526458877', '1526458878', '1526458879', '1526458880', '1526458881', '1526458882', '1526458883', '1526458884', '1526458885', '1526458886', '1526458887', '1526458888', '1526458889', '1526458890', '1526458891', '1526458892', '1526458893', '1526458894', '1526458895', '1526458896', '1526458897', '1526458898', '1526458899', '1526458900', '1526458901', '1526458902', '1526458903', '1526458904', '1526458905', '1526458906', '1526458907', '1526458908', '1526458909', '1526458910', '1526458911', '1526458912', '1526458913', '1526458914', '1526458915', '1526458916', '1526458917', '1526458918', '1526458919', '1526458920', '1526458921', '1526458922', '1526458923', '1526458924', '1526458925', '1526458926', '1526458927', '1526458928', '1526458929', '1526458930', '1526458931', '1526458932', '1526458933', '1526458934', '1526458935', '1526458936', '1526458937', '1526458938', '1526458939', '1526458940', '1526458941', '1526458942', '1526458943', '1526458944', '1526458945', '1526458946', '1526458947', '1526458948', '1526458949', '1526458950', '1526458951', '1526458952', '1526458953', '1526458954', '1526458955', '1526458956', '1526458957', '1526458958', '1526458959', '1526458960', '1526458961', '1526458962', '1526458963', '1526458964', '1526458965', '1526458966', '1526458967', '1526458968', '1526458969', '1526458970', '1526458971', '1526458972', '1526458973', '1526458974', '1526458975', '1526458976', '1526458977', '1526458978', '1526458979', '1526458980', '1526458981', '1526458982', '1526458983', '1526458984', '1526458985', '1526458986', '1526458987', '1526458988', '1526458989', '1526458990', '1526458991', '1526458992', '1526458993', '1526458994', '1526458995', '1526458996', '1526458997', '1526458998', '1526458999', '1526459000', '1526459001', '1526459002', '1526459003', '1526459004', '1526459005', '1526459006', '1526459007', '1526459008', '1526459009', '1526459010', '1526459011', '1526459012', '1526459013', '1526459014', '1526459015', '1526459016', '1526459017', '1526459018', '1526459019', '1526459020', '1526459021', '1526459022', '1526459023', '1526459024', '1526459025', '1526459026', '1526459027', '1526459028', '1526459029', '1526459030', '1526459031', '1526459032', '1526459033', '1526459034', '1526459035', '1526459036', '1526459037', '1526459038', '1526459039', '1526459040', '1526459041', '1526459042', '1526459043', '1526459044', '1526459045', '1526459046', '1526459047', '1526459048', '1526459049', '1526459050', '1526459051', '1526459052', '1526459053', '1526459054', '1526459055', '1526459056', '1526459057', '1526459058', '1526459059', '1526459060', '1526459061', '1526459062', '1526459063', '1526459064', '1526459065', '1526459066', '1526459067', '1526459068']\n",
      "totally cost 1s\n",
      "tf-idf model has beens build successfully\n",
      "similarity success\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "import word_segmentation\n",
    "\n",
    "\n",
    "def load_1h_news():\n",
    "    \"\"\"\n",
    "    加载过去一小时内的所有新闻\n",
    "    \"\"\"\n",
    "    # 1小时时间窗口的历史新闻数据分词结果列表[title+content,],时间戳列表[timestamp]\n",
    "    all_doc_list = []\n",
    "    all_timestamp_list = []\n",
    "\n",
    "    # 数据库通过时间范围查询获取一小时的新闻纪录放入all_doc[timestamp+title+content,]\n",
    "    # 测试代码，使用../data/news0312.csv作为一小时时间窗口的历史新闻数据\n",
    "    ts = '1526458569'\n",
    "    \n",
    "    with open('/data/jupyter/stock/data/news_init.csv', 'r', encoding='utf-8') as data:\n",
    "        for line in data:\n",
    "            row_words = line.replace('\\n', '').split(',')\n",
    "            news_ts = ts\n",
    "            ts = str(int(ts) + 1)\n",
    "            all_doc_list.append(row_words)\n",
    "            all_timestamp_list.append(news_ts)\n",
    "    print(\"load 1h news success\")\n",
    "    print(all_timestamp_list)\n",
    "\n",
    "    return all_doc_list, all_timestamp_list\n",
    "\n",
    "\n",
    "class NewsSimilarity(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.__all_doc_list, self.__all_timestamp_list = load_1h_news()\n",
    "        time_start = time.time()\n",
    "        self.__dictionary = corpora.Dictionary(self.__all_doc_list)\n",
    "        self.__corpus = [self.__dictionary.doc2bow(doc) for doc in self.__all_doc_list]\n",
    "        self.__tfidf = models.TfidfModel(self.__corpus)\n",
    "        time_elapsed = time.time() - time_start\n",
    "        print('totally cost {:.0f}s'.format(time_elapsed))\n",
    "        print(\"tf-idf model has beens build successfully\")\n",
    "\n",
    "    def news_similarity(self, news_seg):\n",
    "        \"\"\"\n",
    "        读取一条新闻的标题和正文，计算该新闻与历史新闻数据的相似度并标记\n",
    "        :param news_seg: 新闻的标题和正文分词结果（list）\n",
    "        :return: repeat 是否重复的标记（int）\n",
    "        \"\"\"\n",
    "\n",
    "        # 使用doc2bow将新推送的新闻转换为二元组的向量\n",
    "        news_vec = self.__dictionary.doc2bow(news_seg)\n",
    "\n",
    "        # 对corpus语料库中的每个目标文档计算文档的相似度\n",
    "        index = similarities.SparseMatrixSimilarity(self.__tfidf[self.__corpus], num_features=len(self.__dictionary.keys()))\n",
    "        sim = index[self.__tfidf[news_vec]]\n",
    "        print(\"similarity success\")\n",
    "\n",
    "        # 如果有相似度在90%以上的文档存在，即将新推送的新闻标记为1，否则0\n",
    "        repeat = 1 if sim.max(axis=0) >= 0.95 else 0\n",
    "\n",
    "        return repeat\n",
    "\n",
    "    def add_news(self, news_timestamp, news_seg):\n",
    "        \"\"\"\n",
    "        将新推送的新闻添加至历史数据列表及语料库中\n",
    "        \"\"\"\n",
    "        # 新增推送新闻至all_doc_list（一小时时间窗口历史数据分词结果列表）\n",
    "        self.__all_timestamp_list.append(news_timestamp)\n",
    "        self.__all_doc_list.append(news_seg)\n",
    "        # 新增推送新闻所建立的向量至corpus语料库\n",
    "        self.__corpus.append(self.__dictionary.doc2bow(news_seg))\n",
    "\n",
    "    def delete_1h_news(self):\n",
    "        \"\"\"\n",
    "        定时（每小时）删除all_doc_list里前一小时的新闻数据\n",
    "        \"\"\"\n",
    "        time_flag = int(self.__all_timestamp_list[-1]) - 3600\n",
    "        print(time_flag)\n",
    "\n",
    "        index = 0\n",
    "        for ts in self.__all_timestamp_list:\n",
    "            if int(ts) >= time_flag:\n",
    "                del_news = index\n",
    "                break\n",
    "            else:\n",
    "                index += 1\n",
    "\n",
    "        del self.__all_timestamp_list[0:del_news]\n",
    "        del self.__all_doc_list[0:del_news]\n",
    "        print(self.__all_timestamp_list)\n",
    "        \n",
    "    \n",
    "    def tf_idf_model(self):\n",
    "        \"\"\"\n",
    "        重新计算tf-idf模型\n",
    "        \"\"\"\n",
    "        self.__dictionary = corpora.Dictionary(self.__all_doc_list)\n",
    "        self.__corpus = [self.__dictionary.doc2bow(doc) for doc in self.__all_doc_list]\n",
    "        self.__tfidf = models.TfidfModel(self.__corpus)\n",
    "        print(\"tf-idf model has beens build successfully\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #time_start = time.time()\n",
    "    # 新推送的新闻\n",
    "    str_title_content = \"孙公司海航地产拟与海南融创昌晟签订《股权转让协议》，出售海航地产所持有的海岛物流100%的股权，转让价款约7.97亿元；同时，海航地产拟出售所持有的海南高和房地产开发有限公司100%的股权至海南融创昌晟，转让价款约11.36亿元。责任编辑：张恒\"\n",
    "\n",
    "    # 新闻分词\n",
    "    ws = word_segmentation.WordSegmentation()\n",
    "    news_seg = ws.word_segmentation(str_title_content)\n",
    "    news_timestamp = '1526523278'\n",
    "    print(\"分词结果：\")\n",
    "    print(news_seg)\n",
    "\n",
    "    # 加载NewsSimilarity类\n",
    "    news_similarity = NewsSimilarity()\n",
    "\n",
    "    repeat = news_similarity.news_similarity(news_seg)\n",
    "\n",
    "    #time_elapsed = time.time() - time_start\n",
    "    #print('totally cost {:.0f}s'.format(time_elapsed))\n",
    "\n",
    "    # 将新推送的新闻添加至历史数据列表及语料库中\n",
    "    news_similarity.add_news(news_timestamp, news_seg)\n",
    "    print(repeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词结果：\n",
      "['孙', '公司', '海航', '地产', '拟', '海南', '融创昌晟', '签订', '股权', '转让', '协议', '出售', '海航', '地产', '持有', '海岛', '物流', '股权', '转让', '价款', '约', '亿元', '海航', '地产', '拟', '出售', '持有', '海南', '高', '房地', '地产', '房地产', '开发', '有限', '公司', '有限公司', '股权', '海南', '融创昌晟', '转让', '价款', '约', '亿元', '责任', '编辑', '责任编辑', '周翎羽']\n",
      "similarity success\n",
      "totally cost 1s\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "# 新推送的新闻\n",
    "str_title_content = \"孙公司海航地产拟与海南融创昌晟签订《股权转让协议》，出售海航地产所持有的海岛物流100%的股权，转让价款约7.97亿元；同时，海航地产拟出售所持有的海南高和房地产开发有限公司100%的股权至海南融创昌晟，转让价款约11.36亿元。责任编辑：周翎羽\"\n",
    "\n",
    "# 新闻分词\n",
    "ws = word_segmentation.WordSegmentation()\n",
    "news_seg = ws.word_segmentation(str_title_content)\n",
    "news_timestamp = '1526523316'\n",
    "\n",
    "print(\"分词结果：\")\n",
    "print(news_seg)\n",
    "\n",
    "repeat = news_similarity.news_similarity(news_seg)\n",
    "\n",
    "time_elapsed = time.time() - time_start\n",
    "print('totally cost {:.0f}s'.format(time_elapsed))\n",
    "\n",
    "# 将新推送的新闻添加至历史数据列表及语料库中\n",
    "news_similarity.add_news(news_timestamp, news_seg)\n",
    "print(repeat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1526519716\n",
      "['1526523278', '1526523316']\n"
     ]
    }
   ],
   "source": [
    "news_similarity.delete_1h_news()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
