{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新闻爬取xlxs文件目录：\n",
    "NEWS_PATH = '../news/'\n",
    "#语料库文件路径：\n",
    "DATA_PATH = '../data/news_words.txt'\n",
    "#标记文件路径\n",
    "LABEL_PATH = '../data/news_labels.txt'\n",
    "#标记数据路径\n",
    "LABEL_DIR = '../labels/'\n",
    "# LDA Model 路径：\n",
    "LDA_PATH = '../model/lda.model'\n",
    "# Dictionary 路径\n",
    "DICT_PATH = '../model/dictionary.txtdic'\n",
    "# Random Forest 模型路径\n",
    "RF_PATH = '../model/rf.model'\n",
    "#coding=utf-8  \n",
    "import codecs  \n",
    "from gensim import corpora  \n",
    "from gensim.models import LdaModel  \n",
    "from gensim.corpora import Dictionary  \n",
    "\n",
    "\n",
    "import jieba\n",
    "import csv\n",
    "import numpy as np  \n",
    "import os  \n",
    "import pandas as pd\n",
    "from sklearn.naive_bayes import GaussianNB  \n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import time    \n",
    "from sklearn import metrics    \n",
    "import pickle as pickle    \n",
    "import pandas as pd  \n",
    "  \n",
    "import random\n",
    "\n",
    "\n",
    "import shutil\n",
    "import time\n",
    "import word_segmentation as ws\n",
    "word_segmentor = ws.WordSegmentation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 由CSV生成语料库\n",
    "\n",
    "* 读取csv文件\n",
    "* 对于一行资讯进行分词\n",
    "* 将分词结果写入新的txt文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常量定义\n",
    "\n",
    "#语料库文件路径：\n",
    "DATA_PATH = '../data/news_words.txt'\n",
    "\n",
    "import shutil\n",
    "import time\n",
    "import pandas as pd\n",
    "import word_segmentation as ws\n",
    "word_segmentor = ws.WordSegmentation()\n",
    "\n",
    "\n",
    "# write_news_words\n",
    "# 输入 csv 文件的 file_path\n",
    "# 操作：分词-写入DATA_FILE\n",
    "def write_news_words(file_path):\n",
    "    file_data = pd.read_csv(file_path)\n",
    "    file_data.rename(columns={'标题':'title', '正文':'content','正文1':'content',\"字段1_文本\":\"title\"}, inplace = True)\n",
    "    file = open(DATA_PATH,\"a\")\n",
    "    try:\n",
    "        for index, row in file_data.iterrows():\n",
    "            #print(row.content)\n",
    "            row_words = word_segmentor.word_segmentation(str(row.title) + str(row.content))\n",
    "            file_words = \" \".join(row_words)\n",
    "            file.write(file_words + ' \\n')\n",
    "    except:\n",
    "        print(\"Error reading news from the file:\", file)\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "# 添加文件到语料库\n",
    "# 输入：new_path 为新增的抓取csv数据的文件夹\n",
    "# 执行：将csv文件的分词结果写入语料库文件\n",
    "def append_csv2txt(new_path,old_path):\n",
    "    files = os.listdir(new_path)\n",
    "    print(files)\n",
    "    \n",
    "    for fname in files:\n",
    "        fpath = new_path + fname\n",
    "        if 'csv' in fpath:\n",
    "            print(fpath)\n",
    "            write_news_words(fpath)\n",
    "            shutil.move(fpath,old_path)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-ebca84fc8a61>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mappend_csv2txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../news/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../corpus/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading CSV files took %fs!'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-8322efa796c0>\u001b[0m in \u001b[0;36mappend_csv2txt\u001b[0;34m(new_path, old_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# 执行：将csv文件的分词结果写入语料库文件\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mappend_csv2txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mold_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# 将 news 文件夹中的csv文件读取存入语料库，并将csv文件移至 corpus 文件夹中\n",
    "\n",
    "start_time = time.time()\n",
    "append_csv2txt('../news/','../corpus/')\n",
    "print('Loading CSV files took %fs!' % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标记数据集生成\n",
    "\n",
    "对标记数据csv进行读取-分词 的转换，并将结果连同label一起写入txt文件\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# write_news_words\n",
    "# 输入 csv 文件的 file_path\n",
    "# 操作：分词-写入DATA_FILE\n",
    "def write_news_labels(fpath):\n",
    "    print(\"Reading label csv file:\",fpath)\n",
    "    file_data = pd.read_csv(fpath)\n",
    "    file_data.rename(columns={'标题':'title', '正文':'content','正文1':'content',\"字段1_文本\":\"title\",\"关键词\":\"plate\"}, inplace = True)\n",
    "    file = open(LABEL_PATH,\"a\")\n",
    "    try:\n",
    "        for index, row in file_data.iterrows():\n",
    "            #print(row.content)\n",
    "            #news_word = jieba_split(str(row.title) + str(row.content))\n",
    "            news_word = \" \".join(word_segmentor.word_segmentation(str(row.title) + str(row.content)))\n",
    "            #news_bow = dictionary.doc2bow(news_word)      #文档转换成bow  \n",
    "            #news_lda = lda2list(lda[news_bow],301) #得到lda向量\n",
    "            file.write(row.plate + \",\" + news_word + \"\\n\")\n",
    "            #print(row.plate,news_lda)\n",
    "    except:\n",
    "        print(\"Error reading labels from the file:\", fpath)\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "# 添加文件到语料库\n",
    "# 输入：new_path 为新增的抓取csv数据的文件夹\n",
    "# 执行：将csv文件的分词结果写入语料库文件\n",
    "def append_labels2txt(new_path,old_path):\n",
    "    files = os.listdir(new_path)\n",
    "    print(files)\n",
    "    for fname in files:\n",
    "        fpath = new_path + fname\n",
    "        if 'csv' in fpath:\n",
    "            print(fpath)\n",
    "            write_news_labels(fpath)\n",
    "            shutil.move(fpath,old_path)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['东财关键词对应新闻-0426-6.csv', '东方财经关键词板块对应新闻-0425-2.csv', '东方财经关键词板块对应新闻-0425-1.csv', '东财关键词对应新闻-0427.csv', '行业研究报告数据中心东方财富网-0502-2.csv', '行业研究报告数据中心东方财富网-0502-1.csv', '东财关键词对应新闻-0426-7.csv', '东财关键词对应新闻-0426-4.csv', '东财关键词对应新闻-0426-3.csv', '东财关键词对应新闻-0426-8.csv', '东方财经关键词板块对应新闻-0427.csv', '东财关键词对应新闻-0426-1.csv', '东财关键词对应新闻-0426-5.csv', '东财关键词对应新闻-0426-2.csv']\n",
      "../labels/东财关键词对应新闻-0426-6.csv\n",
      "Reading label csv file: ../labels/东财关键词对应新闻-0426-6.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.890 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error reading labels from the file: ../labels/东财关键词对应新闻-0426-6.csv\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "Destination path '../label_old/东财关键词对应新闻-0426-6.csv' already exists",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5b198997b34d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mappend_labels2txt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../labels/'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'../label_old/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading CSV files took %fs!'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-2c3351470256>\u001b[0m in \u001b[0;36mappend_labels2txt\u001b[0;34m(new_path, old_path)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mwrite_news_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mold_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    534\u001b[0m         \u001b[0mreal_dst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 536\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination path '%s' already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    537\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    538\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: Destination path '../label_old/东财关键词对应新闻-0426-6.csv' already exists"
     ]
    }
   ],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "append_labels2txt('../labels/','../label_old/')\n",
    "print('Loading CSV files took %fs!' % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 离线语料分析建模及板块预测模型优化\n",
    "\n",
    "LDA：\n",
    "\n",
    "* 生成语料库（初期由csv文件生成，后期由数据集中获取）\n",
    "* 读取语料库（每行一条资讯，已分词，空格分隔）\n",
    "* 利用语料库训练LDA模型并保存\n",
    "\n",
    "RF：\n",
    "\n",
    "* 读取标记数据库（每行一条资讯，第一列为标签，第二列为资讯，已分词，空格分隔）\n",
    "* 利用LDA模型转换资讯为Vector-标记 格式\n",
    "* 训练RF模型并保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 语料库载入及LDA模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in open(DATA_PATH):\n",
    "            # assume there's one document per line, tokens separated by whitespace\n",
    "            yield dictionary.doc2bow(line.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start reading corpus file...\n",
      "Composing dictionary took 269.134172s!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inesa/.local/lib/python3.5/site-packages/gensim/models/ldamodel.py:775: RuntimeWarning: divide by zero encountered in log\n",
      "  diff = np.log(self.expElogbeta)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA training took 11300.780501s!\n"
     ]
    }
   ],
   "source": [
    "# 遍历语料库文件，逐步增加dictionary\n",
    "print(\"Start reading corpus file...\")\n",
    "start_time = time.time()    \n",
    "fr=open(DATA_PATH,'r')  \n",
    "train=[]  \n",
    "corpus_mem_friendly = MyCorpus()\n",
    "# dictionary = corpora.Dictionary(corpus_mem_friendly)\n",
    "dictionary = corpora.Dictionary(line.split(' ') for line in open(DATA_PATH))\n",
    "print('Composing dictionary took %fs!' % (time.time() - start_time)) \n",
    "start_time = time.time()\n",
    "lda = LdaModel(corpus=corpus_mem_friendly, id2word=dictionary, num_topics=400)  \n",
    "lda.save(LDA_PATH)\n",
    "print('LDA training took %fs!' % (time.time() - start_time)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save(DICT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(124, '0.390*\"银行\" + 0.055*\"中国\" + 0.039*\"商银\" + 0.031*\"民生\" + 0.026*\"分行\" + 0.024*\"生银\" + 0.024*\"工商\" + 0.024*\"平安\" + 0.022*\"民生银行\" + 0.021*\"中国银行\"')\n",
      "(256, '0.039*\"市场\" + 0.023*\"投资\" + 0.018*\"估值\" + 0.018*\"A股\" + 0.017*\"成长\" + 0.016*\"行情\" + 0.016*\"机会\" + 0.012*\"蓝筹\" + 0.012*\"股\" + 0.011*\"资金\"')\n",
      "(251, '0.079*\"持股\" + 0.064*\"流通\" + 0.049*\"社保\" + 0.047*\"十大\" + 0.045*\"个股\" + 0.045*\"万股\" + 0.035*\"比例\" + 0.033*\"持有\" + 0.029*\"流通股\" + 0.026*\"新进\"')\n",
      "(230, '0.234*\"美元\" + 0.163*\"亿美元\" + 0.041*\"美国\" + 0.030*\"公司\" + 0.024*\"万美元\" + 0.024*\"年\" + 0.018*\"全球\" + 0.017*\"亚马\" + 0.017*\"亚马逊\" + 0.015*\"超过\"')\n",
      "(175, '0.040*\"成本\" + 0.036*\"税收\" + 0.033*\"降低\" + 0.030*\"政策\" + 0.028*\"所得\" + 0.027*\"增值\" + 0.027*\"企业\" + 0.025*\"增值税\" + 0.024*\"收费\" + 0.023*\"所得税\"')\n",
      "(203, '0.094*\"工厂\" + 0.078*\"自动\" + 0.068*\"自动化\" + 0.058*\"生产\" + 0.043*\"模具\" + 0.042*\"工业\" + 0.038*\"供应\" + 0.034*\"制造\" + 0.028*\"供应商\" + 0.028*\"订单\"')\n",
      "(320, '0.089*\"汽车\" + 0.021*\"车型\" + 0.020*\"上汽\" + 0.019*\"大众\" + 0.019*\"汽车产业\" + 0.019*\"自主\" + 0.018*\"销量\" + 0.018*\"品牌\" + 0.017*\"一汽\" + 0.015*\"长安\"')\n",
      "(38, '0.143*\"证监\" + 0.127*\"证监会\" + 0.034*\"证券\" + 0.025*\"处罚\" + 0.021*\"中国证监会\" + 0.020*\"行政\" + 0.017*\"月\" + 0.017*\"年\" + 0.016*\"信息\" + 0.016*\"披露\"')\n",
      "(301, '0.219*\"新闻\" + 0.097*\"发布会\" + 0.077*\"发言\" + 0.056*\"发言人\" + 0.034*\"科学家\" + 0.030*\"波兰\" + 0.026*\"学家\" + 0.026*\"座\" + 0.024*\"加州\" + 0.021*\"发布\"')\n",
      "(209, '0.272*\"养老\" + 0.053*\"人口\" + 0.042*\"老龄\" + 0.035*\"社区\" + 0.034*\"老龄化\" + 0.031*\"退休\" + 0.028*\"社会\" + 0.022*\"养老保险\" + 0.022*\"生育\" + 0.020*\"老人\"')\n",
      "(309, '0.068*\"互动\" + 0.053*\"联络\" + 0.052*\"资金流\" + 0.048*\"乐普医疗\" + 0.037*\"联络互动\" + 0.025*\"地产股\" + 0.025*\"黄河\" + 0.024*\"贡酒\" + 0.021*\"洋河股份\" + 0.021*\"古井\"')\n",
      "(386, '0.045*\"社会\" + 0.033*\"公共\" + 0.030*\"服务\" + 0.025*\"设施\" + 0.024*\"建设\" + 0.023*\"保障\" + 0.020*\"基础\" + 0.019*\"城乡\" + 0.017*\"就业\" + 0.017*\"基础设施\"')\n",
      "(198, '0.270*\"香港\" + 0.074*\"内地\" + 0.068*\"港元\" + 0.031*\"亿港元\" + 0.021*\"年\" + 0.018*\"国际\" + 0.018*\"蚂蚁\" + 0.016*\"金服\" + 0.013*\"港币\" + 0.012*\"区政\"')\n",
      "(241, '0.087*\"上市\" + 0.078*\"IPO\" + 0.039*\"A股\" + 0.034*\"发行\" + 0.028*\"上交\" + 0.026*\"上交所\" + 0.023*\"新\" + 0.019*\"退市\" + 0.018*\"市场\" + 0.017*\"企业\"')\n",
      "(341, '0.045*\"公司\" + 0.043*\"亏损\" + 0.038*\"年\" + 0.018*\"资产\" + 0.018*\"损益\" + 0.016*\"减值\" + 0.016*\"会计\" + 0.015*\"计提\" + 0.014*\"合并\" + 0.014*\"年度\"')\n",
      "(54, '0.076*\"减持\" + 0.064*\"股东\" + 0.061*\"增持\" + 0.053*\"万股\" + 0.052*\"公司\" + 0.051*\"股份\" + 0.040*\"月\" + 0.038*\"占\" + 0.031*\"股本\" + 0.028*\"总\"')\n",
      "(359, '0.210*\"阿里\" + 0.165*\"腾讯\" + 0.102*\"阿里巴巴\" + 0.092*\"百度\" + 0.027*\"巨头\" + 0.021*\"云\" + 0.020*\"步步\" + 0.019*\"步步高\" + 0.011*\"系\" + 0.011*\"BAT\"')\n",
      "(349, '0.278*\"券商\" + 0.079*\"两融\" + 0.034*\"证券\" + 0.033*\"证券公司\" + 0.031*\"配资\" + 0.030*\"场外\" + 0.025*\"杠杆\" + 0.022*\"保证金\" + 0.017*\"保证\" + 0.015*\"投行\"')\n",
      "(360, '0.166*\"期货\" + 0.055*\"市场\" + 0.054*\"交易\" + 0.040*\"期权\" + 0.028*\"现货\" + 0.023*\"期货市场\" + 0.020*\"商品\" + 0.020*\"投保\" + 0.019*\"合约\" + 0.019*\"品种\"')\n",
      "(55, '0.204*\"苏州\" + 0.092*\"人文\" + 0.090*\"创投\" + 0.066*\"中巴\" + 0.055*\"哲学\" + 0.043*\"宗教\" + 0.027*\"猛进\" + 0.023*\"党派\" + 0.020*\"宏伟\" + 0.019*\"马拉\"')\n",
      "(52, '0.312*\"报告\" + 0.082*\"财务\" + 0.078*\"年度\" + 0.040*\"审计\" + 0.037*\"会计\" + 0.032*\"年度报告\" + 0.026*\"指标\" + 0.019*\"事务\" + 0.019*\"年\" + 0.019*\"事务所\"')\n",
      "(348, '0.129*\"珠宝\" + 0.063*\"L\" + 0.056*\"第一名\" + 0.039*\"首饰\" + 0.038*\"二号\" + 0.028*\"永续\" + 0.028*\"珠宝首饰\" + 0.028*\"股票投资\" + 0.025*\"克莱\" + 0.024*\"无关\"')\n",
      "(157, '0.047*\"行业\" + 0.034*\"有望\" + 0.023*\"板块\" + 0.021*\"龙头\" + 0.019*\"推荐\" + 0.018*\"关注\" + 0.017*\"投资\" + 0.017*\"预期\" + 0.017*\"受益\" + 0.016*\"看好\"')\n",
      "(70, '0.088*\"涨停\" + 0.040*\"沪\" + 0.037*\"两市\" + 0.034*\"个股\" + 0.034*\"涨幅\" + 0.024*\"板块\" + 0.022*\"亿元\" + 0.022*\"点\" + 0.020*\"股份\" + 0.020*\"股\"')\n",
      "(273, '0.195*\"宁波\" + 0.154*\"储能\" + 0.091*\"离子\" + 0.080*\"照明\" + 0.068*\"锂离子\" + 0.047*\"天地\" + 0.025*\"波音\" + 0.022*\"国贸\" + 0.017*\"演示\" + 0.016*\"金龙\"')\n",
      "(64, '0.110*\"轮胎\" + 0.074*\"贝尔\" + 0.056*\"B股\" + 0.053*\"中泰\" + 0.051*\"橡胶\" + 0.030*\"三聚\" + 0.027*\"化学\" + 0.027*\"黑猫\" + 0.027*\"三聚环保\" + 0.024*\"粘胶\"')\n",
      "(199, '0.370*\"项目\" + 0.061*\"PPP\" + 0.046*\"投资\" + 0.045*\"建设\" + 0.017*\"模式\" + 0.017*\"设施\" + 0.014*\"基础\" + 0.013*\"基础设施\" + 0.013*\"资本\" + 0.012*\"工程\"')\n",
      "(324, '0.055*\"市场\" + 0.016*\"月\" + 0.014*\"资金\" + 0.013*\"股价\" + 0.011*\"时间\" + 0.008*\"投资者\" + 0.007*\"大幅\" + 0.007*\"历史\" + 0.007*\"近期\" + 0.007*\"高\"')\n",
      "(128, '0.151*\"出行\" + 0.122*\"滴滴\" + 0.098*\"出租\" + 0.068*\"租车\" + 0.061*\"司机\" + 0.056*\"美团\" + 0.050*\"相通\" + 0.039*\"出租车\" + 0.030*\"长途\" + 0.020*\"车辆\"')\n",
      "(297, '0.084*\"国家\" + 0.040*\"管理\" + 0.040*\"负责\" + 0.037*\"职责\" + 0.030*\"部\" + 0.028*\"机构\" + 0.024*\"资源\" + 0.023*\"部门\" + 0.023*\"二十\" + 0.022*\"分工\"')\n",
      "(208, '0.056*\"通信\" + 0.055*\"5G\" + 0.031*\"导体\" + 0.031*\"半导\" + 0.031*\"半导体\" + 0.020*\"产业\" + 0.018*\"芯片\" + 0.018*\"设备\" + 0.017*\"宽带\" + 0.015*\"全球\"')\n",
      "(144, '0.024*\"升机\" + 0.024*\"康佳\" + 0.023*\"长征\" + 0.022*\"直升\" + 0.022*\"救援\" + 0.021*\"直升机\" + 0.019*\"运载\" + 0.018*\"艘\" + 0.017*\"大洋\" + 0.015*\"泰山\"')\n",
      "(399, '0.071*\"建设\" + 0.031*\"经济带\" + 0.023*\"开发\" + 0.022*\"城市\" + 0.017*\"高压\" + 0.016*\"供电\" + 0.015*\"城际\" + 0.015*\"发展\" + 0.015*\"特高\" + 0.014*\"布局\"')\n",
      "(85, '0.367*\"业务\" + 0.115*\"管理\" + 0.070*\"客户\" + 0.024*\"资管\" + 0.021*\"规模\" + 0.019*\"产品\" + 0.017*\"转型\" + 0.013*\"发展\" + 0.009*\"行业\" + 0.007*\"新规\"')\n",
      "(131, '0.259*\"a\" + 0.098*\"b\" + 0.085*\"全文\" + 0.072*\"点击\" + 0.060*\"阅读\" + 0.047*\"提示\" + 0.026*\"责任\" + 0.025*\"个股\" + 0.024*\"编辑\" + 0.023*\"责任编辑\"')\n",
      "(0, '0.143*\"板\" + 0.064*\"东\" + 0.060*\"信达\" + 0.042*\"建军\" + 0.037*\"中投\" + 0.036*\"发审委\" + 0.035*\"发审\" + 0.032*\"华西\" + 0.030*\"换人\" + 0.028*\"一届\"')\n",
      "(155, '0.251*\"并购\" + 0.065*\"收购\" + 0.047*\"重组\" + 0.046*\"海外\" + 0.030*\"整合\" + 0.021*\"产业\" + 0.016*\"股权\" + 0.014*\"跨界\" + 0.013*\"案例\" + 0.012*\"金额\"')\n",
      "(93, '0.028*\"板块\" + 0.027*\"市场\" + 0.015*\"点\" + 0.015*\"大盘\" + 0.014*\"个股\" + 0.014*\"反弹\" + 0.013*\"震荡\" + 0.013*\"今日\" + 0.010*\"行情\" + 0.010*\"强势\"')\n",
      "(194, '0.266*\"智能\" + 0.109*\"人工\" + 0.097*\"人工智能\" + 0.031*\"技术\" + 0.023*\"领域\" + 0.023*\"AI\" + 0.021*\"智能化\" + 0.019*\"科技\" + 0.017*\"计算\" + 0.015*\"识别\"')\n",
      "(53, '0.047*\"中\" + 0.013*\"方法\" + 0.013*\"一种\" + 0.012*\"大部\" + 0.012*\"过程\" + 0.011*\"大部分\" + 0.011*\"选择\" + 0.010*\"之间\" + 0.009*\"方式\" + 0.009*\"时\"')\n",
      "第一主题\n",
      "0.142*\"医疗\" + 0.106*\"器械\" + 0.093*\"医疗器械\" + 0.053*\"中新\" + 0.024*\"新网\" + 0.024*\"通科技\" + 0.023*\"国安\" + 0.018*\"格林美\" + 0.017*\"恒通\" + 0.017*\"贝因美\"\n"
     ]
    }
   ],
   "source": [
    "# 打印LDA模型相关结果\n",
    "topic_list=lda.print_topics(40)  \n",
    "\n",
    "for topic in topic_list:  \n",
    "    print(topic) \n",
    "print(\"第一主题\")\n",
    "print(lda.print_topic(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 标签库载入及分类器训练\n",
    "\n",
    "* 标记数据来源： 第一财经新闻等按关键词搜索的新闻爬取结果\n",
    "* 标记格式：新闻标题/正文/关键词\n",
    "\n",
    "训练过程：\n",
    "\n",
    "* 读取含有“关键词”-“标题”-“正文” 的csv文件\n",
    "* 对文件中的 标题 + 正文 字符串进行分词\n",
    "* 分词结果使用LDA模型向量化\n",
    "* 向量 ： 关键词 作为 Random Forest模型的训练输入\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LDA model took 2.170682s!\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "lda = LdaModel.load(LDA_PATH)\n",
    "print('Loading LDA model took %fs!' % (time.time() - start_time)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading labeled data...\n",
      "['东财关键词对应新闻-0426-6.csv', '东方财经关键词板块对应新闻-0425-2.csv', '东方财经关键词板块对应新闻-0425-1.csv', '东财关键词对应新闻-0427.csv', '行业研究报告数据中心东方财富网-0502-2.csv', '行业研究报告数据中心东方财富网-0502-1.csv', '东财关键词对应新闻-0426-7.csv', '东财关键词对应新闻-0426-4.csv', '东财关键词对应新闻-0426-3.csv', '东财关键词对应新闻-0426-8.csv', '东方财经关键词板块对应新闻-0427.csv', '东财关键词对应新闻-0426-1.csv', '东财关键词对应新闻-0426-5.csv', '东财关键词对应新闻-0426-2.csv']\n",
      "../labels/东财关键词对应新闻-0426-6.csv\n",
      "done\n",
      "../labels/东方财经关键词板块对应新闻-0425-2.csv\n",
      "done\n",
      "../labels/东方财经关键词板块对应新闻-0425-1.csv\n",
      "done\n",
      "../labels/东财关键词对应新闻-0427.csv\n",
      "done\n",
      "../labels/行业研究报告数据中心东方财富网-0502-2.csv\n",
      "done\n",
      "../labels/行业研究报告数据中心东方财富网-0502-1.csv\n",
      "done\n",
      "../labels/东财关键词对应新闻-0426-7.csv\n",
      "done\n",
      "../labels/东财关键词对应新闻-0426-4.csv\n",
      "done\n",
      "../labels/东财关键词对应新闻-0426-3.csv\n",
      "done\n",
      "../labels/东财关键词对应新闻-0426-8.csv\n",
      "done\n",
      "../labels/东方财经关键词板块对应新闻-0427.csv\n",
      "done\n",
      "../labels/东财关键词对应新闻-0426-1.csv\n",
      "done\n",
      "../labels/东财关键词对应新闻-0426-5.csv\n",
      "done\n",
      "../labels/东财关键词对应新闻-0426-2.csv\n",
      "done\n",
      "Loading labeled data took 43836.135314s!\n"
     ]
    }
   ],
   "source": [
    "import word_segmentation\n",
    "# 对于label文件夹下的CSV，读取后将标题+正文转换为分词结果，再转换为LDA结果，形成200维向量\n",
    "# 对于新闻的标签，直接保留其表格中的“关键词”地段\n",
    "\n",
    "corpus_mem_friendly = MyCorpus()\n",
    "# dictionary = corpora.Dictionary(corpus_mem_friendly)\n",
    "dictionary = corpora.Dictionary(line.split(' ') for line in open(DATA_PATH))\n",
    "\n",
    "#df_label = pd.DataFrame(columns=['words','label'])  \n",
    "news_vec = []\n",
    "news_labels = []\n",
    "\n",
    "# turn lda result into list\n",
    "def lda2list(lda,topic_n):\n",
    "    lda_dict = dict(lda)\n",
    "    lda_list = [0] * topic_n\n",
    "    for i in range(topic_n):\n",
    "        lda_list[i] = lda_dict.get(i,0)\n",
    "    return lda_list\n",
    "\n",
    "start_time = time.time()\n",
    "print(\"Start loading labeled data...\")\n",
    "files = os.listdir(LABEL_DIR)\n",
    "print(files)\n",
    "for fname in files:\n",
    "    fpath = LABEL_DIR + fname\n",
    "    if 'csv' in fpath:\n",
    "        print(fpath)\n",
    "        file_data = pd.read_csv(fpath)\n",
    "        file_data.rename(columns={'标题':'title', '正文':'content','正文1':'content',\"字段1_文本\":\"title\",\"关键词\":\"plate\"}, inplace = True)\n",
    "        for index, row in file_data.iterrows():\n",
    "            #print(row.content)\n",
    "            #news_word = jieba_split(str(row.title) + str(row.content))\n",
    "            news_word = word_segmentor.word_segmentation(str(row.title) + str(row.content))\n",
    "            news_bow = dictionary.doc2bow(news_word)      #文档转换成bow  \n",
    "            news_lda = lda2list(lda[news_bow],301) #得到lda向量\n",
    "            news_vec.append(news_lda)\n",
    "            news_labels.append(row.plate)\n",
    "            #print(row.plate,news_lda)\n",
    "    print(\"done\")\n",
    "print('Loading labeled data took %fs!' % (time.time() - start_time)) \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********classifier: random forest ***********\n",
      "training took 40.354848s!\n",
      "['文化传媒']\n",
      "材料行业\n",
      "['珠宝首饰']\n",
      "珠宝首饰\n",
      "['高速公路']\n",
      "高速公路\n",
      "['电子信息']\n",
      "电子信息\n",
      "['公用事业']\n",
      "公用事业\n",
      "['公用事业']\n",
      "公用事业\n",
      "['保险']\n",
      "银行\n",
      "['保险']\n",
      "保险\n",
      "['贵金属']\n",
      "贵金属\n",
      "['房地产']\n",
      "房地产\n",
      "['贵金属']\n",
      "贵金属\n",
      "['电子信息']\n",
      "电子信息\n",
      "['高速公路']\n",
      "高速公路\n",
      "['石油行业']\n",
      "石油行业\n",
      "['港口水运']\n",
      "船舶制造\n",
      "['银行']\n",
      "银行\n",
      "['家电行业']\n",
      "家电行业\n",
      "['医疗行业']\n",
      "医疗行业\n",
      "['高速公路']\n",
      "高速公路\n",
      "['汽车行业']\n",
      "汽车行业\n",
      "['保险']\n",
      "保险\n",
      "['文教休闲']\n",
      "文教休闲\n",
      "['文化传媒']\n",
      "文化传媒\n",
      "['家电行业']\n",
      "汽车行业\n",
      "['汽车行业']\n",
      "汽车行业\n",
      "['电子信息']\n",
      "高速公路\n",
      "['银行']\n",
      "银行\n",
      "['工艺商品']\n",
      "工艺商品\n",
      "['农药兽药']\n",
      "文教休闲\n",
      "['电子信息']\n",
      "电子信息\n",
      "['公用事业']\n",
      "公用事业\n",
      "['纺织服装']\n",
      "纺织服装\n",
      "['珠宝首饰']\n",
      "珠宝首饰\n",
      "['贵金属']\n",
      "贵金属\n",
      "['汽车行业']\n",
      "汽车行业\n",
      "['材料行业']\n",
      "材料行业\n",
      "['高速公路']\n",
      "高速公路\n",
      "['贵金属']\n",
      "贵金属\n",
      "['化工行业']\n",
      "石油行业\n",
      "['纺织服装']\n",
      "纺织服装\n",
      "['文教休闲']\n",
      "文教休闲\n",
      "['贵金属']\n",
      "贵金属\n",
      "['房地产']\n",
      "房地产\n",
      "['仪器仪表']\n",
      "仪器仪表\n",
      "['汽车行业']\n",
      "汽车行业\n",
      "['房地产']\n",
      "房地产\n",
      "['电子信息']\n",
      "电子信息\n",
      "['有色金属']\n",
      "有色金属\n",
      "['化工行业']\n",
      "化工行业\n",
      "['汽车行业']\n",
      "汽车行业\n",
      "['高速公路']\n",
      "高速公路\n",
      "['汽车行业']\n",
      "汽车行业\n",
      "['电信运营']\n",
      "电信运营\n",
      "['文化传媒']\n",
      "文化传媒\n",
      "['化工行业']\n",
      "化工行业\n",
      "['贵金属']\n",
      "贵金属\n",
      "['机械行业']\n",
      "机械行业\n",
      "['纺织服装']\n",
      "纺织服装\n",
      "['家电行业']\n",
      "家电行业\n",
      "['纺织服装']\n",
      "纺织服装\n",
      "['化工行业']\n",
      "化工行业\n",
      "['煤炭采选']\n",
      "煤炭采选\n",
      "['交运物流']\n",
      "交运物流\n",
      "['公用事业']\n",
      "公用事业\n",
      "['高速公路']\n",
      "高速公路\n",
      "['房地产']\n",
      "软件服务\n",
      "['文化传媒']\n",
      "文化传媒\n",
      "['有色金属']\n",
      "有色金属\n",
      "['家电行业']\n",
      "家电行业\n",
      "['交运物流']\n",
      "交运物流\n",
      "['造纸印刷']\n",
      "造纸印刷\n",
      "['食品行业']\n",
      "食品行业\n",
      "['纺织服装']\n",
      "纺织服装\n",
      "['银行']\n",
      "银行\n",
      "['交运设备']\n",
      "交运设备\n",
      "['工艺商品']\n",
      "工艺商品\n",
      "['综合行业']\n",
      "综合行业\n",
      "['民航机场']\n",
      "民航机场\n",
      "['化工行业']\n",
      "化工行业\n",
      "['公用事业']\n",
      "公用事业\n",
      "['纺织服装']\n",
      "纺织服装\n",
      "['汽车行业']\n",
      "汽车行业\n",
      "['保险']\n",
      "保险\n",
      "['工程建设']\n",
      "工程建设\n",
      "['汽车行业']\n",
      "汽车行业\n",
      "['房地产']\n",
      "房地产\n",
      "['国际贸易']\n",
      "国际贸易\n",
      "['国际贸易']\n",
      "国际贸易\n",
      "['纺织服装']\n",
      "纺织服装\n",
      "['房地产']\n",
      "房地产\n",
      "['高速公路']\n",
      "高速公路\n",
      "['券商信托']\n",
      "券商信托\n",
      "['医疗行业']\n",
      "医疗行业\n",
      "['文化传媒']\n",
      "文化传媒\n",
      "['纺织服装']\n",
      "纺织服装\n",
      "['交运物流']\n",
      "交运物流\n",
      "['高速公路']\n",
      "高速公路\n",
      "['钢铁行业']\n",
      "钢铁行业\n",
      "['保险']\n",
      "银行\n",
      "['房地产']\n",
      "房地产\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "# Random Forest Classifier    \n",
    "def random_forest_classifier(train_x, train_y):    \n",
    "    from sklearn.ensemble import RandomForestClassifier    \n",
    "    model = RandomForestClassifier(n_estimators=8)    \n",
    "    model.fit(train_x, train_y)    \n",
    "    return model    \n",
    "    \n",
    "\n",
    "X = np.array([np.array(xi) for xi in news_vec])\n",
    "Y = np.array(news_labels)\n",
    "\n",
    "print(\"********classifier: random forest ***********\")\n",
    "start_time = time.time()    \n",
    "model = random_forest_classifier(X, Y)\n",
    "print('training took %fs!' % (time.time() - start_time)) \n",
    "for i in range(100):\n",
    "    n = random.randrange(1,len(X))\n",
    "    # predict_proba(x)：给出带有概率值的结果。每个点在所有label的概率和为1.  \n",
    "    # predict(x)：直接给出预测结果。内部还是调用的predict_proba()，根据概率的结果看哪个类型的预测值最高就是哪个类型。  \n",
    "    print(model.predict([X[n]]))\n",
    "    print(Y[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../model/rf.model']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, RF_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
