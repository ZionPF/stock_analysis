{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 财经新闻分析与分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常量定义\n",
    "# 新闻爬取xlxs文件目录：\n",
    "NEWS_PATH = '../news/'\n",
    "\n",
    "#语料库文件路径：\n",
    "DATA_PATH = '../data/news_words.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 财经新闻抓取数据导入\n",
    "\n",
    "对于爬虫抓取的数据，首先将Excel格式文件转换为 utf-8 编码的CSV文件格式。\n",
    "\n",
    "\n",
    "格式：CSV文件\n",
    "* 新闻标题\n",
    "* 新闻时间\n",
    "* 新闻正文\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os  \n",
    "import os.path  \n",
    "  \n",
    "#这里放着你要操作的文件夹名称  \n",
    "#datapath = '../news/'  \n",
    "\n",
    "# 将给定路径path的xlsx 文件转换为CSV 文件，并存在相同目录下\n",
    "def xlsx_to_csv_pd(path):\n",
    "    if 'xlsx' in path:\n",
    "        csvPath = path.replace('xlsx','csv')\n",
    "    elif 'xls' in path:\n",
    "        csvPath = path.replace('xls','csv')\n",
    "    else:\n",
    "        return None\n",
    "    try:\n",
    "        data_xls = pd.read_excel(path, index_col=0)\n",
    "        data_xls.to_csv(csvPath, encoding='utf-8')\n",
    "    except:\n",
    "        print('Error changing the following file from xls to csv:',path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#把目录下的文件名全部获取保存在files中  \n",
    "files = os.listdir(NEWS_PATH)  \n",
    "for fname in files:\n",
    "    fpath = NEWS_PATH + fname\n",
    "    xlsx_to_csv_pd(fpath)\n",
    "xlsx_to_csv_pd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## csv 文档分词后转换\n",
    "读取所有CSV文件中的新闻，对于每条内容进行分词后，写成新文件的一行，加以存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "# 加载停用词，输入停用词文件，输出停用词list\n",
    "def stopwordslist(filepath):  \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]  \n",
    "    return stopwords\n",
    "\n",
    "temp_stop_list = ['\\u3000','\\xa0','\\t']\n",
    "stop_words = stopwordslist(\"../utils/stopwords.txt\") + temp_stop_list\n",
    "# 为结巴分词词库加载股票名词汇\n",
    "jieba.load_userdict('../data/user_dict.txt')\n",
    "\n",
    "# check if a string s is a number\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    return False\n",
    " \n",
    "## jieba分词：输入string & 停用词文件，输出分词结果list\n",
    "def jieba_split(content):\n",
    "    '''\n",
    "    content: 输入文本（string）\n",
    "    stop_path: 停用词字典文件路径（string）\n",
    "    返回：list，jieba分词结果\n",
    "    '''\n",
    "    str_content = str(content).replace('\\t', '').replace('\\n', '').replace(' ','')\n",
    "    str_words = ','.join(jieba.cut_for_search(str_content)).split(\",\")\n",
    "    ret_list = []\n",
    "    for word in str_words:\n",
    "        if word not in stop_words:\n",
    "            if word[-1] != '%':\n",
    "                ret_list.append(word)\n",
    "    return ret_list\n",
    "\n",
    "\n",
    "# 测试：读取一个csv文件，将列名替换：“标题”：title，‘正文’：content\n",
    "\n",
    "# write_news_words\n",
    "# 输入 csv 文件的 file_path\n",
    "# 操作：分词-写入DATA_FILE\n",
    "def write_news_words(file_path):\n",
    "    file_data = pd.read_csv(file_path)\n",
    "    file_data.rename(columns={'标题':'title', '正文':'content','正文1':'content',\"字段1_文本\":\"title\"}, inplace = True)\n",
    "    file = open(DATA_PATH,\"a\")\n",
    "    for index, row in file_data.iterrows():\n",
    "        #print(row.content)\n",
    "        row_words = jieba_split(str(row.title) + str(row.content))\n",
    "        file_words = \" \".join(row_words)\n",
    "        file.write(file_words + ' \\n')\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "    \n",
    "# 添加文件到语料库\n",
    "# 输入：new_path 为新增的抓取csv数据的文件夹\n",
    "# 执行：将csv文件的分词结果写入语料库文件\n",
    "def append_csv2txt(new_path):\n",
    "    files = os.listdir(new_path)\n",
    "    print(files)\n",
    "    for fname in files:\n",
    "        fpath = new_path + fname\n",
    "        if 'csv' in fpath:\n",
    "            print(fpath)\n",
    "            write_news_words(fpath)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['东财关键词对应新闻-0427.csv', '东方财经关键词板块对应新闻-0427.csv']\n",
      "../tmp/东财关键词对应新闻-0427.csv\n",
      "../tmp/东方财经关键词板块对应新闻-0427.csv\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "#append_csv2txt('../news/')\n",
    "#append_csv2txt('../labels/')\n",
    "append_csv2txt('../tmp/')\n",
    "print(\"Finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = pd.read_csv(\"../labels/第一财经板块对应新闻-0416.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入语料库数据\n",
    "\n",
    "读取对应的语料库文件，将每一行读入后形成list，载入doc2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "\n",
    "采用 Doc2Vec 方法将一段文字转换成对应的向量，各文本之间的差别可以用向量夹脚进行计算。\n",
    "Doc2Vec 的目的是获得文档的一个固定长度的向量表达。\n",
    "\n",
    "数据：多个文档，以及它们的标签，可以用标题作为标签。 \n",
    "影响模型准确率的因素：语料的大小，文档的数量，越多越高；文档的相似性，越相似越好。\n",
    "\n",
    "优势：Doc2Vec是无监督学习，不需要具体标注，可以用文档名作为标签。\n",
    "注意事项：训练集越大，结果越好。语料库中各文章越相似，结果越好\n",
    "\n",
    "\n",
    "使用方式：gensim 的 doc2vec \n",
    "\n",
    "后续工作：对于各文章的向量，首先可以进行基于语义的归类、相似度计算。其次可以将向量与关键词进行匹配，方法可以是标记训练，也可以是关键词 -- 搜索文本 -- doc2Vec ，之后计算向量夹角。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analyzedDocument' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0a234d0b022e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analyzedDocument' is not defined"
     ]
    }
   ],
   "source": [
    "import jieba  \n",
    "import sys  \n",
    "from collections import namedtuple\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "\n",
    "docs = []\n",
    "\n",
    "    \n",
    "file = open(DATA_PATH,'r')\n",
    "for (tag,line) in enumerate(file):\n",
    "    words = line.split(\" \")\n",
    "    words.remove(\"\\n\")\n",
    "    tags = [tag]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)\n",
    "\n",
    "# Get the vectors\n",
    "print(model.dovcecs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a8dd37e3534d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model.docvecs[1000]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#model.infer_vector(['大数据'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# model.docvecs[0]\n",
    "# model.docvecs[1000]\n",
    "#model.infer_vector(['大数据'])\n",
    "len(model.docvecs)\n",
    "\n",
    "\n",
    "def test():  \n",
    "    model_dm = model \n",
    "    test_text = docs[13586].words\n",
    "    #test_text = [\"区块链\"]\n",
    "    print(test_text)\n",
    "    inferred_vector_dm = model_dm.infer_vector(test_text)  \n",
    "    print(inferred_vector_dm)\n",
    "    sims = model_dm.docvecs.most_similar([inferred_vector_dm], topn=10)  \n",
    "    return sims\n",
    "    \n",
    "\n",
    "sims = test()  \n",
    "\n",
    "for count, sim in sims:  \n",
    "    sentence = docs[count]  \n",
    "    words = ''  \n",
    "    for word in sentence[0]:  \n",
    "        words = words + word + ''  \n",
    "    print(sim)\n",
    "    print(words, sim, len(sentence[0]))\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 板块概念关键词获取\n",
    "通过同花顺网站 http://q.10jqka.com.cn/gn/ ，获取概念关键词列表。\n",
    "后续列表将根据华鑫的语料库和客户的需求进行扩充和更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['阿里巴巴概念', '安防', '白酒', '白马股', '参股保险', '参股民营银行', '参股券商', '参股360', '参股新三板', '草甘膦', '超导', '超级品牌', '车联网', '充电桩', '创投', '大飞机', '大数据', '锂电池', '电子发票', '电子竞技', '电子商务', '电子信息', '迪士尼', '地下管网', '电力改革', '东盟自贸区', '二胎概念', '二维码识别', '风电', '分散染料', '氟化工', '福建自贸区', '高端装备', '高送转', '高铁', '高校', '工业4.0', '供应链金融', '股权转让', '广东自贸区', '光伏概念', '固废处理', '国产软件', '军工', '共享单车', '海工装备', '航运', '杭州亚运会', '核电', '互联网+', '互联网彩票', '互联网金融', '互联网医疗', '黄金', '沪港通概念', '互联网保险', '金融IC', '集成电路', '建筑节能', '家用电器', '节能环保', '节能照明', '金改', '京津冀一体化', '机器人概念', '基因测序', '健康中国', '军民融合', '举牌', '可燃冰', '跨境电商', '宽带中国', '蓝宝石', '冷链物流', '两桶油改革', '量子通信', '生态农业', '蚂蚁金服概念', '煤化工', '美丽中国', '民营医院', 'MSCI概念', '马云概念', '能源互联网', '农村电商', '农机', '农业现代化', 'O2O概念', 'OLED', 'P2P概念', '啤酒', '苹果概念', 'PM2.5', 'PPP概念', '汽车电子', '期货概念', '氢燃料电池', '禽流感', '区块链', '人工智能', '人脸识别', '融资融券', '乳业', '上海国资改革', '上海自贸区', '深港通', '生物医药', '石墨电极', '石墨烯', '食品安全', '首发新股', '手机游戏', '水利', '水泥', 'ST板块', '生物质能', '深圳国资改革', '钛白粉', '太阳能', '碳纤维', '特钢', '特高压', '腾讯概念', '特色小镇', '特斯拉', '天津自贸区', '天然气', '体育产业', '通用航空', '土地流转', '脱硫脱硝', '万达私有化', '网络游戏', '网约车', '王者荣耀', '尾气治理', '卫星导航', '文化传媒', '物联网', '物流电商平台', '无人机', '无人驾驶', '无人零售', '污水处理', '无线充电', '微信小程序', '雄安新区', '西安自贸区', '消费金融', '小金属', '细胞免疫治疗', '新材料概念', '新股与次新股', '新疆振兴', '新能源', '新能源汽车', '芯片概念', '网络安全', '稀缺资源', '稀土永磁', '虚拟现实', '养老概念', '央企国资改革', '页岩气', '一带一路', '移动互联网', '移动支付', '医疗改革', '医疗器械', '油品改革', '油品升级', '粤港澳概念', '云计算', '语音技术', '医药电商', '在线教育', '在线旅游', '债转股', '振兴东北', '智慧城市', '智能穿戴', '智能电网', '智能家居', '智能交通', '智能物流', '智能医疗', '智能音箱', '职业教育', '中韩自贸区', '中字头股票', '转融券标的', '猪肉', '证金持股', '摘帽', '装配式建筑', '足球概念', '租售同权', '自由贸易港', '3D打印', '4G5G']\n"
     ]
    }
   ],
   "source": [
    "# Get notion list\n",
    "# From: http://q.10jqka.com.cn/gn/\n",
    "import re\n",
    "\n",
    "file_object = open('../data/tonghuashun_gn.txt')\n",
    "try:\n",
    "     gn_text = file_object.read( )\n",
    "finally:\n",
    "     file_object.close()\n",
    "\n",
    "gn_list = re.findall(r\">(.+?)</a>\", gn_text)\n",
    "print(gn_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新闻分词\n",
    "\n",
    "我们使用jieba分词对每条新闻进行分词处理。\n",
    "\n",
    "输入：文本（string)\n",
    "输出：分词结果（list）\n",
    "\n",
    "后续需要进行的工作包括：\n",
    "\n",
    "* 增加财经类专有词汇进入jieba的词典\n",
    "* 增加停用词词典，以过滤无效分词。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count\n",
    "\n",
    "# wordcount方法，输入分词结果的list，对于每个单词进行数量统计\n",
    "\n",
    "def wordcount(word_list):\n",
    "    '''\n",
    "    input: l as a list of strings\n",
    "    output: dict of {word:count}\n",
    "    '''\n",
    "    count = {}\n",
    "    for i in word_list:\n",
    "        if i in count.keys():\n",
    "            count[i] += 1\n",
    "        else:\n",
    "            count[i] = 1\n",
    "    return count\n",
    "\n",
    "# word_key_count 方法，输入分词结果list，对于特定的keywords中的词语进行统计（非keyword直接忽略）\n",
    "\n",
    "def word_key_count(wordlist,keywords):\n",
    "    '''\n",
    "    input: wordlist as a list of strings\n",
    "        keywords as a list of keywords\n",
    "    output: dict of {keyword:count}\n",
    "    '''\n",
    "    count = {}\n",
    "    for i in wordlist:\n",
    "        if i in keywords:\n",
    "            if i in count.keys():\n",
    "                count[i] += 1\n",
    "            else:\n",
    "                count[i] = 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'债转股': 5682, '电子商务': 3002, '在线教育': 1132, '生物质能': 139, '稀缺资源': 286, '集成电路': 3597, '特斯拉': 7089, '蓝宝石': 326, '禽流感': 131, '家用电器': 4422, '网约车': 194, '超导': 398, '文化传媒': 1001, '核电': 6585, '手机游戏': 102, '锂电池': 3288, '草甘膦': 370, 'OLED': 1017, '太阳能': 2185, '特钢': 6309, '迪士尼': 2185, '黄金': 17328, '农机': 1602, '人工智能': 7496, '碳纤维': 261, '白酒': 4906, '可燃冰': 227, 'PM2.5': 387, '电子竞技': 140, '网络安全': 5330, '无人驾驶': 1685, '4G5G': 6, '建筑节能': 78, '创投': 4642, '虚拟现实': 1305, '污水处理': 869, '智能家居': 1073, '无人机': 1747, '人脸识别': 591, '电子信息': 2931, '水利': 5150, '啤酒': 39109, '国产软件': 1950, '军工': 41670, '生态农业': 126, '食品安全': 661, '举牌': 8110, '煤化工': 819, '安防': 1516, '金改': 86, '高铁': 3674, '航运': 3060, '特高压': 1395, '网络游戏': 435, '乳业': 3756, '石墨电极': 170, '水泥': 16772, '风电': 4390, '钛白粉': 1107, '分散染料': 107, '摘帽': 723, '天然气': 4591, '医疗器械': 1860, '猪肉': 697, '体育产业': 1911, '生物医药': 1216, '新能源': 24052, '白马股': 3082, '高校': 1130}\n"
     ]
    }
   ],
   "source": [
    "## 对于目前的所有新闻，每条新闻进行关键词对应，查看关键词覆盖情况\n",
    "\n",
    "file = open(DATA_PATH,'r')\n",
    "\n",
    "#news_gn = pd.DataFrame(columns = [\"news_id\"] + gn_list)\n",
    "\n",
    "for (tag,line) in enumerate(file):\n",
    "    words = line.split(\" \")\n",
    "    for word in words:\n",
    "        if word in gn_list:\n",
    "            if word in gn_count.keys():\n",
    "                gn_count[word] += 1\n",
    "            else:\n",
    "                gn_count[word] = 1\n",
    "    #key_count = word_key_count(words,gn_list)\n",
    "    #print(key_count)\n",
    "    #print(\"----------\")\n",
    "print(gn_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "    require.config({\n",
       "         paths:{\n",
       "           'echarts': '/nbextensions/echarts/echarts.min'\n",
       "         }\n",
       "    });\n",
       "</script>\n",
       "<div id=\"5d853e516a734a71a10b2ae6da7690f3\" style=\"width:800px; height:400px;\"></div>\n",
       "\n",
       "<script>\n",
       "    require([ 'echarts' ],function(ec){\n",
       "\tvar myChart = ec.init(document.getElementById('5d853e516a734a71a10b2ae6da7690f3'));\n",
       "var option =  {\n",
       "    \"color\": [\n",
       "        \"#c23531\",\n",
       "        \"#2f4554\",\n",
       "        \"#61a0a8\",\n",
       "        \"#d48265\",\n",
       "        \"#749f83\",\n",
       "        \"#ca8622\",\n",
       "        \"#bda29a\",\n",
       "        \"#6e7074\",\n",
       "        \"#546570\",\n",
       "        \"#c4ccd3\",\n",
       "        \"#f05b72\",\n",
       "        \"#ef5b9c\",\n",
       "        \"#f47920\",\n",
       "        \"#905a3d\",\n",
       "        \"#fab27b\",\n",
       "        \"#2a5caa\",\n",
       "        \"#444693\",\n",
       "        \"#726930\",\n",
       "        \"#b2d235\",\n",
       "        \"#6d8346\",\n",
       "        \"#ac6767\",\n",
       "        \"#1d953f\",\n",
       "        \"#6950a1\",\n",
       "        \"#918597\",\n",
       "        \"#f6f5ec\"\n",
       "    ],\n",
       "    \"toolbox\": {\n",
       "        \"feature\": {\n",
       "            \"restore\": {\n",
       "                \"show\": true\n",
       "            },\n",
       "            \"saveAsImage\": {\n",
       "                \"title\": \"\\u4e0b\\u8f7d\\u56fe\\u7247\",\n",
       "                \"show\": true\n",
       "            },\n",
       "            \"dataView\": {\n",
       "                \"show\": true\n",
       "            }\n",
       "        },\n",
       "        \"left\": \"95%\",\n",
       "        \"top\": \"center\",\n",
       "        \"orient\": \"vertical\",\n",
       "        \"show\": true\n",
       "    },\n",
       "    \"series\": [\n",
       "        {\n",
       "            \"name\": \"precipitation\",\n",
       "            \"type\": \"bar\",\n",
       "            \"barCategoryGap\": \"20%\",\n",
       "            \"label\": {\n",
       "                \"emphasis\": {\n",
       "                    \"textStyle\": {\n",
       "                        \"color\": \"#fff\",\n",
       "                        \"fontSize\": 12\n",
       "                    },\n",
       "                    \"position\": null,\n",
       "                    \"show\": true\n",
       "                },\n",
       "                \"normal\": {\n",
       "                    \"formatter\": null,\n",
       "                    \"textStyle\": {\n",
       "                        \"color\": \"#000\",\n",
       "                        \"fontSize\": 12\n",
       "                    },\n",
       "                    \"position\": \"top\",\n",
       "                    \"show\": false\n",
       "                }\n",
       "            },\n",
       "            \"seriesId\": 2559129,\n",
       "            \"markPoint\": {\n",
       "                \"data\": [\n",
       "                    {\n",
       "                        \"name\": \"Maximum\",\n",
       "                        \"type\": \"max\",\n",
       "                        \"label\": {\n",
       "                            \"normal\": {\n",
       "                                \"textStyle\": {\n",
       "                                    \"color\": \"#fff\"\n",
       "                                }\n",
       "                            }\n",
       "                        },\n",
       "                        \"symbolSize\": 50,\n",
       "                        \"symbol\": \"pin\",\n",
       "                        \"valueDim\": null\n",
       "                    },\n",
       "                    {\n",
       "                        \"name\": \"Minimum\",\n",
       "                        \"type\": \"min\",\n",
       "                        \"label\": {\n",
       "                            \"normal\": {\n",
       "                                \"textStyle\": {\n",
       "                                    \"color\": \"#fff\"\n",
       "                                }\n",
       "                            }\n",
       "                        },\n",
       "                        \"symbolSize\": 50,\n",
       "                        \"symbol\": \"pin\",\n",
       "                        \"valueDim\": null\n",
       "                    }\n",
       "                ]\n",
       "            },\n",
       "            \"data\": [\n",
       "                3805,\n",
       "                2016,\n",
       "                756,\n",
       "                93,\n",
       "                193,\n",
       "                2412,\n",
       "                4735,\n",
       "                218,\n",
       "                89,\n",
       "                2956,\n",
       "                130,\n",
       "                266,\n",
       "                670,\n",
       "                4403,\n",
       "                69,\n",
       "                2204,\n",
       "                247,\n",
       "                682,\n",
       "                1459,\n",
       "                4213,\n",
       "                1457,\n",
       "                11642,\n",
       "                1070,\n",
       "                5078,\n",
       "                176,\n",
       "                3307,\n",
       "                152,\n",
       "                259,\n",
       "                94,\n",
       "                3566,\n",
       "                1126,\n",
       "                4,\n",
       "                52,\n",
       "                3103,\n",
       "                871,\n",
       "                580,\n",
       "                717,\n",
       "                1170,\n",
       "                397,\n",
       "                1955,\n",
       "                3438,\n",
       "                26119,\n",
       "                1305,\n",
       "                27827,\n",
       "                84,\n",
       "                443,\n",
       "                5418,\n",
       "                548,\n",
       "                1018,\n",
       "                59,\n",
       "                2466,\n",
       "                2048,\n",
       "                931,\n",
       "                294,\n",
       "                2506,\n",
       "                120,\n",
       "                11196,\n",
       "                2954,\n",
       "                739,\n",
       "                72,\n",
       "                495,\n",
       "                3090,\n",
       "                1244,\n",
       "                468,\n",
       "                1274,\n",
       "                822,\n",
       "                16121,\n",
       "                2093,\n",
       "                759\n",
       "            ],\n",
       "            \"stack\": \"\",\n",
       "            \"markLine\": {\n",
       "                \"data\": [\n",
       "                    {\n",
       "                        \"name\": \"mean-Value\",\n",
       "                        \"type\": \"average\",\n",
       "                        \"valueDim\": null\n",
       "                    }\n",
       "                ],\n",
       "                \"symbolSize\": 10\n",
       "            }\n",
       "        }\n",
       "    ],\n",
       "    \"tooltip\": {\n",
       "        \"axisPointer\": {\n",
       "            \"type\": \"line\"\n",
       "        },\n",
       "        \"textStyle\": {\n",
       "            \"color\": \"#fff\",\n",
       "            \"fontSize\": 14\n",
       "        },\n",
       "        \"formatter\": null,\n",
       "        \"triggerOn\": \"mousemove|click\",\n",
       "        \"trigger\": \"item\"\n",
       "    },\n",
       "    \"series_id\": 2559129,\n",
       "    \"backgroundColor\": \"#fff\",\n",
       "    \"yAxis\": [\n",
       "        {\n",
       "            \"nameGap\": 25,\n",
       "            \"axisLabel\": {\n",
       "                \"margin\": 8,\n",
       "                \"formatter\": \"{value} \",\n",
       "                \"textStyle\": {\n",
       "                    \"color\": \"#000\",\n",
       "                    \"fontSize\": 12\n",
       "                },\n",
       "                \"interval\": \"auto\",\n",
       "                \"rotate\": 0\n",
       "            },\n",
       "            \"nameLocation\": \"middle\",\n",
       "            \"show\": true,\n",
       "            \"name\": \"\",\n",
       "            \"type\": \"value\",\n",
       "            \"min\": null,\n",
       "            \"max\": null,\n",
       "            \"nameTextStyle\": {\n",
       "                \"fontSize\": 14\n",
       "            },\n",
       "            \"inverse\": false,\n",
       "            \"axisTick\": {\n",
       "                \"alignWithLabel\": false\n",
       "            },\n",
       "            \"position\": null,\n",
       "            \"boundaryGap\": true\n",
       "        }\n",
       "    ],\n",
       "    \"title\": [\n",
       "        {\n",
       "            \"subtext\": \"precipitation and evaporation one year\",\n",
       "            \"subtextStyle\": {\n",
       "                \"color\": \"#aaa\",\n",
       "                \"fontSize\": 12\n",
       "            },\n",
       "            \"top\": \"auto\",\n",
       "            \"text\": \"Bar chart\",\n",
       "            \"left\": \"auto\",\n",
       "            \"textStyle\": {\n",
       "                \"color\": \"#000\",\n",
       "                \"fontSize\": 18\n",
       "            }\n",
       "        }\n",
       "    ],\n",
       "    \"xAxis\": [\n",
       "        {\n",
       "            \"nameGap\": 25,\n",
       "            \"axisLabel\": {\n",
       "                \"margin\": 8,\n",
       "                \"textStyle\": {\n",
       "                    \"color\": \"#000\",\n",
       "                    \"fontSize\": 12\n",
       "                },\n",
       "                \"rotate\": 0,\n",
       "                \"interval\": \"auto\"\n",
       "            },\n",
       "            \"nameLocation\": \"middle\",\n",
       "            \"show\": true,\n",
       "            \"name\": \"\",\n",
       "            \"type\": \"category\",\n",
       "            \"min\": null,\n",
       "            \"max\": null,\n",
       "            \"nameTextStyle\": {\n",
       "                \"fontSize\": 14\n",
       "            },\n",
       "            \"inverse\": false,\n",
       "            \"data\": [\n",
       "                \"\\u503a\\u8f6c\\u80a1\",\n",
       "                \"\\u7535\\u5b50\\u5546\\u52a1\",\n",
       "                \"\\u5728\\u7ebf\\u6559\\u80b2\",\n",
       "                \"\\u751f\\u7269\\u8d28\\u80fd\",\n",
       "                \"\\u7a00\\u7f3a\\u8d44\\u6e90\",\n",
       "                \"\\u96c6\\u6210\\u7535\\u8def\",\n",
       "                \"\\u7279\\u65af\\u62c9\",\n",
       "                \"\\u84dd\\u5b9d\\u77f3\",\n",
       "                \"\\u79bd\\u6d41\\u611f\",\n",
       "                \"\\u5bb6\\u7528\\u7535\\u5668\",\n",
       "                \"\\u7f51\\u7ea6\\u8f66\",\n",
       "                \"\\u8d85\\u5bfc\",\n",
       "                \"\\u6587\\u5316\\u4f20\\u5a92\",\n",
       "                \"\\u6838\\u7535\",\n",
       "                \"\\u624b\\u673a\\u6e38\\u620f\",\n",
       "                \"\\u9502\\u7535\\u6c60\",\n",
       "                \"\\u8349\\u7518\\u81a6\",\n",
       "                \"OLED\",\n",
       "                \"\\u592a\\u9633\\u80fd\",\n",
       "                \"\\u7279\\u94a2\",\n",
       "                \"\\u8fea\\u58eb\\u5c3c\",\n",
       "                \"\\u9ec4\\u91d1\",\n",
       "                \"\\u519c\\u673a\",\n",
       "                \"\\u4eba\\u5de5\\u667a\\u80fd\",\n",
       "                \"\\u78b3\\u7ea4\\u7ef4\",\n",
       "                \"\\u767d\\u9152\",\n",
       "                \"\\u53ef\\u71c3\\u51b0\",\n",
       "                \"PM2.5\",\n",
       "                \"\\u7535\\u5b50\\u7ade\\u6280\",\n",
       "                \"\\u7f51\\u7edc\\u5b89\\u5168\",\n",
       "                \"\\u65e0\\u4eba\\u9a7e\\u9a76\",\n",
       "                \"4G5G\",\n",
       "                \"\\u5efa\\u7b51\\u8282\\u80fd\",\n",
       "                \"\\u521b\\u6295\",\n",
       "                \"\\u865a\\u62df\\u73b0\\u5b9e\",\n",
       "                \"\\u6c61\\u6c34\\u5904\\u7406\",\n",
       "                \"\\u667a\\u80fd\\u5bb6\\u5c45\",\n",
       "                \"\\u65e0\\u4eba\\u673a\",\n",
       "                \"\\u4eba\\u8138\\u8bc6\\u522b\",\n",
       "                \"\\u7535\\u5b50\\u4fe1\\u606f\",\n",
       "                \"\\u6c34\\u5229\",\n",
       "                \"\\u5564\\u9152\",\n",
       "                \"\\u56fd\\u4ea7\\u8f6f\\u4ef6\",\n",
       "                \"\\u519b\\u5de5\",\n",
       "                \"\\u751f\\u6001\\u519c\\u4e1a\",\n",
       "                \"\\u98df\\u54c1\\u5b89\\u5168\",\n",
       "                \"\\u4e3e\\u724c\",\n",
       "                \"\\u7164\\u5316\\u5de5\",\n",
       "                \"\\u5b89\\u9632\",\n",
       "                \"\\u91d1\\u6539\",\n",
       "                \"\\u9ad8\\u94c1\",\n",
       "                \"\\u822a\\u8fd0\",\n",
       "                \"\\u7279\\u9ad8\\u538b\",\n",
       "                \"\\u7f51\\u7edc\\u6e38\\u620f\",\n",
       "                \"\\u4e73\\u4e1a\",\n",
       "                \"\\u77f3\\u58a8\\u7535\\u6781\",\n",
       "                \"\\u6c34\\u6ce5\",\n",
       "                \"\\u98ce\\u7535\",\n",
       "                \"\\u949b\\u767d\\u7c89\",\n",
       "                \"\\u5206\\u6563\\u67d3\\u6599\",\n",
       "                \"\\u6458\\u5e3d\",\n",
       "                \"\\u5929\\u7136\\u6c14\",\n",
       "                \"\\u533b\\u7597\\u5668\\u68b0\",\n",
       "                \"\\u732a\\u8089\",\n",
       "                \"\\u4f53\\u80b2\\u4ea7\\u4e1a\",\n",
       "                \"\\u751f\\u7269\\u533b\\u836f\",\n",
       "                \"\\u65b0\\u80fd\\u6e90\",\n",
       "                \"\\u767d\\u9a6c\\u80a1\",\n",
       "                \"\\u9ad8\\u6821\"\n",
       "            ],\n",
       "            \"axisTick\": {\n",
       "                \"alignWithLabel\": false\n",
       "            },\n",
       "            \"position\": null,\n",
       "            \"boundaryGap\": true\n",
       "        }\n",
       "    ],\n",
       "    \"legend\": [\n",
       "        {\n",
       "            \"top\": \"top\",\n",
       "            \"data\": [\n",
       "                \"precipitation\"\n",
       "            ],\n",
       "            \"show\": true,\n",
       "            \"textStyle\": {\n",
       "                \"color\": \"#333\",\n",
       "                \"fontSize\": 12\n",
       "            },\n",
       "            \"left\": \"center\",\n",
       "            \"orient\": \"horizontal\",\n",
       "            \"selectedMode\": \"multiple\"\n",
       "        }\n",
       "    ]\n",
       "};\n",
       "myChart.setOption(option);\n",
       "\n",
       "    });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<pyecharts.charts.bar.Bar at 0x7fdae775c128>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyecharts import Bar\n",
    "\n",
    "attr = list(gn_count.keys())\n",
    "v1 = list(gn_count.values())\n",
    "bar = Bar(\"Bar chart\", \"precipitation and evaporation one year\")\n",
    "bar.add(\"precipitation\", attr, v1, mark_line=[\"average\"], mark_point=[\"max\", \"min\"])\n",
    "bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'dict_items' and 'dict_items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-4612eddd98bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'集成电路'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'军工'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'人工智能'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'新能源'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'高校'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'集成电路'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdictMerged1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'dict_items' and 'dict_items'"
     ]
    }
   ],
   "source": [
    "d1 = {'集成电路': 1}\n",
    "d2 = {'军工': 1, '人工智能': 1, '新能源': 2, '高校': 1, '集成电路': 1}\n",
    "\n",
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.833 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "## jieba\n",
    "# imput：string\n",
    "# output: list of words\n",
    "\n",
    "import jieba  \n",
    "\n",
    "news_list = test_data.loc[1:10,].content.tolist()\n",
    "for news in news_list:\n",
    "    news = str(news).replace('\\t', '').replace('\\n', '').replace(' ','')\n",
    "    #seg = jieba.cut(news, cut_all=False)\n",
    "    #print((\" \".join(seg)))\n",
    "    news_jieba = ','.join(jieba.cut_for_search(news)).split(\",\")\n",
    "    #news_words = ','.join(jieba.cut_for_search(news)).split(\",\")\n",
    "    news_keywords = word_key_count(news_jieba,gn_list)\n",
    "    #print(news_keywords)\n",
    "    #print(\"-----------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述结果可以看到，同花顺的板块关键词与新闻中能够分词得到的关键词之间有很大的距离，很多新闻不包含板块列表中的任何一个关键词。因而，不能单纯的以同花顺板块关键词作为文章的特征向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inesa/.local/lib/python3.5/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "/home/inesa/.local/lib/python3.5/site-packages/ipykernel_launcher.py:49: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word '农业银行公告，拟以非公开发行方式发行不超过274.73亿股A股' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-437d4763702a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"农业银行公告，拟以非公开发行方式发行不超过274.73亿股A股\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1396\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 )\n\u001b[0;32m-> 1398\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordEmbeddingsKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \"\"\"\n\u001b[0;32m--> 696\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '农业银行公告，拟以非公开发行方式发行不超过274.73亿股A股' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "import jieba  \n",
    "import sys  \n",
    "from collections import namedtuple\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "\n",
    "news_contents = test_data.loc[1:10,].content.tolist()\n",
    "news_titles = test_data.loc[1:10,].title.tolist()\n",
    "# 加载停用词\n",
    "def stopwordslist(filepath):  \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]  \n",
    "    return stopwords  \n",
    "stop_words = stopwordslist(\"../utils/stopwords.txt\")\n",
    "\n",
    "\n",
    "def jieba_split(content):\n",
    "    '''\n",
    "    content: 输入string\n",
    "    返回：list，jieba分词结果\n",
    "    '''\n",
    "    str_content = str(content).replace('\\t', '').replace('\\n', '').replace(' ','')\n",
    "    words = ','.join(jieba.cut_for_search(news)).split(\",\")\n",
    "    ret_list = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            if word != '\\t':\n",
    "                ret_list.append(word)\n",
    "    return ret_list\n",
    "\n",
    "\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "# 对于读取的csv中的每行，title直接作为ID，content进行分词\n",
    "for index, row in test_data.iterrows():\n",
    "    words = jieba_split(row.content)\n",
    "    tags = [index]\n",
    "\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "    \n",
    "model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)\n",
    "\n",
    "# Get the vectors\n",
    "\n",
    "model.docvecs[0]\n",
    "model.docvecs[1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.4359948e-04,  2.4775974e-03, -3.3414126e-03,  2.0195886e-03,\n",
       "        3.1975079e-03, -2.3209287e-03, -4.8111733e-03, -1.0556693e-04,\n",
       "       -4.6941759e-03,  8.6439901e-04, -6.0576486e-04,  3.6755938e-03,\n",
       "        4.7860360e-03, -3.5068085e-03, -1.9612887e-03,  6.6922337e-04,\n",
       "        3.9420077e-03,  2.2369092e-03, -3.0673554e-03,  3.7408150e-03,\n",
       "       -1.4501261e-03,  4.6342192e-03,  2.3098721e-03, -4.4978056e-03,\n",
       "        4.6808324e-03,  4.4101542e-03, -1.8619066e-04,  3.7129037e-03,\n",
       "        1.0040315e-03,  4.2312015e-03,  1.7201813e-03,  2.8457702e-04,\n",
       "        2.2850025e-03, -7.9294865e-04, -1.8799296e-03,  3.7282121e-03,\n",
       "       -3.3937453e-03,  1.0326292e-03, -3.0222947e-03, -3.7595741e-03,\n",
       "       -2.7006185e-03, -1.3564046e-03,  3.5538755e-03, -1.8512841e-03,\n",
       "       -4.6294578e-03, -4.5664459e-03,  3.4132274e-03,  5.4600608e-04,\n",
       "        4.6899985e-03,  2.8150952e-03,  1.4626229e-03,  1.9676671e-03,\n",
       "        4.9300781e-03,  5.0489849e-05, -6.9381681e-04, -2.6175675e-03,\n",
       "       -2.8738368e-03,  1.8589342e-03, -1.6758419e-03, -4.7412124e-03,\n",
       "       -1.6757516e-03,  1.2087821e-03,  4.4598398e-03, -2.8066437e-03,\n",
       "        1.4959322e-03,  2.5511980e-03,  8.6957234e-04, -4.6138209e-03,\n",
       "       -7.3039933e-04, -3.6161735e-03,  2.4709667e-04,  1.2982028e-03,\n",
       "       -1.2230397e-04, -4.2607035e-03, -3.8072371e-03, -4.8361863e-03,\n",
       "        2.1853023e-03,  2.9918891e-03, -1.8474633e-03, -1.6187584e-03,\n",
       "       -1.8993358e-03,  1.5900822e-03, -3.1699864e-03,  1.2803908e-03,\n",
       "       -9.1701996e-04, -4.7037415e-03,  1.1838719e-03, -2.8460259e-03,\n",
       "       -2.4618064e-03,  1.5572514e-03, -1.5108566e-03, -1.1715039e-03,\n",
       "        6.7387024e-05, -2.1543100e-03,  4.9384516e-03, -3.3656592e-04,\n",
       "       -4.2337188e-03,  1.9735340e-03,  4.8039434e-03, -1.4595834e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(['大数据'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "\n",
    "# Load data\n",
    "\n",
    "doc1 = [\"This is a sentence\", \"This is another sentence\"]\n",
    "\n",
    "# Transform data (you can add more data preprocessing steps) \n",
    "\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, text in enumerate(doc1):\n",
    "    words = text.lower().split()\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "# Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "\n",
    "model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)\n",
    "\n",
    "# Get the vectors\n",
    "\n",
    "model.docvecs[0]\n",
    "model.docvecs[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
