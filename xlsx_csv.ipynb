{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 财经新闻分析与分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 常量定义\n",
    "# 新闻爬取xlxs文件目录：\n",
    "NEWS_PATH = '../news/'\n",
    "\n",
    "#语料库文件路径：\n",
    "DATA_PATH = '../data/news_words.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 财经新闻抓取数据导入\n",
    "\n",
    "对于爬虫抓取的数据，首先将Excel格式文件转换为 utf-8 编码的CSV文件格式。\n",
    "\n",
    "\n",
    "格式：CSV文件\n",
    "* 新闻标题\n",
    "* 新闻时间\n",
    "* 新闻正文\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os  \n",
    "import os.path  \n",
    "  \n",
    "#这里放着你要操作的文件夹名称  \n",
    "#datapath = '../news/'  \n",
    "\n",
    "# 将给定路径path的xlsx 文件转换为CSV 文件，并存在相同目录下\n",
    "def xlsx_to_csv_pd(path):\n",
    "    if 'xlsx' in path:\n",
    "        csvPath = path.replace('xlsx','csv')\n",
    "    elif 'xls' in path:\n",
    "        csvPath = path.replace('xls','csv')\n",
    "    else:\n",
    "        return None\n",
    "    try:\n",
    "        data_xls = pd.read_excel(path, index_col=0)\n",
    "        data_xls.to_csv(csvPath, encoding='utf-8')\n",
    "    except:\n",
    "        print('Error changing the following file from xls to csv:',path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xlsx_to_csv_pd(\"../data/news0312.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "xlsx_to_csv_pd() missing 1 required positional argument: 'path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5298f3f54b8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../data/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mxlsx_to_csv_pd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mxlsx_to_csv_pd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: xlsx_to_csv_pd() missing 1 required positional argument: 'path'"
     ]
    }
   ],
   "source": [
    "\n",
    "#把目录下的文件名全部获取保存在files中  \n",
    "files = os.listdir(\"../data/\")  \n",
    "for fname in files:\n",
    "    fpath = \"../data/\" + fname\n",
    "    xlsx_to_csv_pd(fpath)\n",
    "xlsx_to_csv_pd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## csv 文档分词后转换\n",
    "读取所有CSV文件中的新闻，对于每条内容进行分词后，写成新文件的一行，加以存储。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import jieba\n",
    "# 加载停用词，输入停用词文件，输出停用词list\n",
    "def stopwordslist(filepath):  \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]  \n",
    "    return stopwords\n",
    "\n",
    "temp_stop_list = ['\\u3000','\\xa0','\\t']\n",
    "stop_words = stopwordslist(\"../utils/stopwords.txt\") + temp_stop_list\n",
    "# 为结巴分词词库加载股票名词汇\n",
    "jieba.load_userdict('../data/user_dict.txt')\n",
    "\n",
    "# check if a string s is a number\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        pass\n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(s)\n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    return False\n",
    " \n",
    "## jieba分词：输入string & 停用词文件，输出分词结果list\n",
    "def jieba_split(content):\n",
    "    '''\n",
    "    content: 输入文本（string）\n",
    "    stop_path: 停用词字典文件路径（string）\n",
    "    返回：list，jieba分词结果\n",
    "    '''\n",
    "    str_content = str(content).replace('\\t', '').replace('\\n', '').replace(' ','')\n",
    "    str_words = ','.join(jieba.cut_for_search(str_content)).split(\",\")\n",
    "    ret_list = []\n",
    "    for word in str_words:\n",
    "        if word not in stop_words:\n",
    "            if word[-1] != '%':\n",
    "                ret_list.append(word)\n",
    "    return ret_list\n",
    "\n",
    "\n",
    "# 测试：读取一个csv文件，将列名替换：“标题”：title，‘正文’：content\n",
    "\n",
    "# write_news_words\n",
    "# 输入 csv 文件的 file_path\n",
    "# 操作：分词-写入DATA_FILE\n",
    "def write_news_words(file_path):\n",
    "    file_data = pd.read_csv(file_path)\n",
    "    file_data.rename(columns={'标题':'title', '正文':'content','正文1':'content',\"字段1_文本\":\"title\"}, inplace = True)\n",
    "    file = open(DATA_PATH,\"a\")\n",
    "    for index, row in file_data.iterrows():\n",
    "        #print(row.content)\n",
    "        row_words = jieba_split(str(row.title) + str(row.content))\n",
    "        file_words = \" \".join(row_words)\n",
    "        file.write(file_words + ' \\n')\n",
    "    file.close()\n",
    "    \n",
    "\n",
    "    \n",
    "# 添加文件到语料库\n",
    "# 输入：new_path 为新增的抓取csv数据的文件夹\n",
    "# 执行：将csv文件的分词结果写入语料库文件\n",
    "def append_csv2txt(new_path):\n",
    "    files = os.listdir(new_path)\n",
    "    print(files)\n",
    "    for fname in files:\n",
    "        fpath = new_path + fname\n",
    "        if 'csv' in fpath:\n",
    "            print(fpath)\n",
    "            write_news_words(fpath)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['东财关键词对应新闻-0427.csv', '东方财经关键词板块对应新闻-0427.csv']\n",
      "../tmp/东财关键词对应新闻-0427.csv\n",
      "../tmp/东方财经关键词板块对应新闻-0427.csv\n",
      "Finish!\n"
     ]
    }
   ],
   "source": [
    "#append_csv2txt('../news/')\n",
    "#append_csv2txt('../labels/')\n",
    "append_csv2txt('../tmp/')\n",
    "print(\"Finish!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_data = pd.read_csv(\"../labels/第一财经板块对应新闻-0416.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 载入语料库数据\n",
    "\n",
    "读取对应的语料库文件，将每一行读入后形成list，载入doc2vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec\n",
    "\n",
    "采用 Doc2Vec 方法将一段文字转换成对应的向量，各文本之间的差别可以用向量夹脚进行计算。\n",
    "Doc2Vec 的目的是获得文档的一个固定长度的向量表达。\n",
    "\n",
    "数据：多个文档，以及它们的标签，可以用标题作为标签。 \n",
    "影响模型准确率的因素：语料的大小，文档的数量，越多越高；文档的相似性，越相似越好。\n",
    "\n",
    "优势：Doc2Vec是无监督学习，不需要具体标注，可以用文档名作为标签。\n",
    "注意事项：训练集越大，结果越好。语料库中各文章越相似，结果越好\n",
    "\n",
    "\n",
    "使用方式：gensim 的 doc2vec \n",
    "\n",
    "后续工作：对于各文章的向量，首先可以进行基于语义的归类、相似度计算。其次可以将向量与关键词进行匹配，方法可以是标记训练，也可以是关键词 -- 搜索文本 -- doc2Vec ，之后计算向量夹角。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'analyzedDocument' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-0a234d0b022e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mwords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mdocs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manalyzedDocument\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdoc2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDoc2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'analyzedDocument' is not defined"
     ]
    }
   ],
   "source": [
    "import jieba  \n",
    "import sys  \n",
    "from collections import namedtuple\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "\n",
    "docs = []\n",
    "\n",
    "    \n",
    "file = open(DATA_PATH,'r')\n",
    "for (tag,line) in enumerate(file):\n",
    "    words = line.split(\" \")\n",
    "    words.remove(\"\\n\")\n",
    "    tags = [tag]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)\n",
    "\n",
    "# Get the vectors\n",
    "print(model.dovcecs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a8dd37e3534d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# model.docvecs[1000]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#model.infer_vector(['大数据'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocvecs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# model.docvecs[0]\n",
    "# model.docvecs[1000]\n",
    "#model.infer_vector(['大数据'])\n",
    "len(model.docvecs)\n",
    "\n",
    "\n",
    "def test():  \n",
    "    model_dm = model \n",
    "    test_text = docs[13586].words\n",
    "    #test_text = [\"区块链\"]\n",
    "    print(test_text)\n",
    "    inferred_vector_dm = model_dm.infer_vector(test_text)  \n",
    "    print(inferred_vector_dm)\n",
    "    sims = model_dm.docvecs.most_similar([inferred_vector_dm], topn=10)  \n",
    "    return sims\n",
    "    \n",
    "\n",
    "sims = test()  \n",
    "\n",
    "for count, sim in sims:  \n",
    "    sentence = docs[count]  \n",
    "    words = ''  \n",
    "    for word in sentence[0]:  \n",
    "        words = words + word + ''  \n",
    "    print(sim)\n",
    "    print(words, sim, len(sentence[0]))\n",
    "    print(\"----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 板块概念关键词获取\n",
    "通过同花顺网站 http://q.10jqka.com.cn/gn/ ，获取概念关键词列表。\n",
    "后续列表将根据华鑫的语料库和客户的需求进行扩充和更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['阿里巴巴概念', '安防', '白酒', '白马股', '参股保险', '参股民营银行', '参股券商', '参股360', '参股新三板', '草甘膦', '超导', '超级品牌', '车联网', '充电桩', '创投', '大飞机', '大数据', '锂电池', '电子发票', '电子竞技', '电子商务', '电子信息', '迪士尼', '地下管网', '电力改革', '东盟自贸区', '二胎概念', '二维码识别', '风电', '分散染料', '氟化工', '福建自贸区', '高端装备', '高送转', '高铁', '高校', '工业4.0', '供应链金融', '股权转让', '广东自贸区', '光伏概念', '固废处理', '国产软件', '军工', '共享单车', '海工装备', '航运', '杭州亚运会', '核电', '互联网+', '互联网彩票', '互联网金融', '互联网医疗', '黄金', '沪港通概念', '互联网保险', '金融IC', '集成电路', '建筑节能', '家用电器', '节能环保', '节能照明', '金改', '京津冀一体化', '机器人概念', '基因测序', '健康中国', '军民融合', '举牌', '可燃冰', '跨境电商', '宽带中国', '蓝宝石', '冷链物流', '两桶油改革', '量子通信', '生态农业', '蚂蚁金服概念', '煤化工', '美丽中国', '民营医院', 'MSCI概念', '马云概念', '能源互联网', '农村电商', '农机', '农业现代化', 'O2O概念', 'OLED', 'P2P概念', '啤酒', '苹果概念', 'PM2.5', 'PPP概念', '汽车电子', '期货概念', '氢燃料电池', '禽流感', '区块链', '人工智能', '人脸识别', '融资融券', '乳业', '上海国资改革', '上海自贸区', '深港通', '生物医药', '石墨电极', '石墨烯', '食品安全', '首发新股', '手机游戏', '水利', '水泥', 'ST板块', '生物质能', '深圳国资改革', '钛白粉', '太阳能', '碳纤维', '特钢', '特高压', '腾讯概念', '特色小镇', '特斯拉', '天津自贸区', '天然气', '体育产业', '通用航空', '土地流转', '脱硫脱硝', '万达私有化', '网络游戏', '网约车', '王者荣耀', '尾气治理', '卫星导航', '文化传媒', '物联网', '物流电商平台', '无人机', '无人驾驶', '无人零售', '污水处理', '无线充电', '微信小程序', '雄安新区', '西安自贸区', '消费金融', '小金属', '细胞免疫治疗', '新材料概念', '新股与次新股', '新疆振兴', '新能源', '新能源汽车', '芯片概念', '网络安全', '稀缺资源', '稀土永磁', '虚拟现实', '养老概念', '央企国资改革', '页岩气', '一带一路', '移动互联网', '移动支付', '医疗改革', '医疗器械', '油品改革', '油品升级', '粤港澳概念', '云计算', '语音技术', '医药电商', '在线教育', '在线旅游', '债转股', '振兴东北', '智慧城市', '智能穿戴', '智能电网', '智能家居', '智能交通', '智能物流', '智能医疗', '智能音箱', '职业教育', '中韩自贸区', '中字头股票', '转融券标的', '猪肉', '证金持股', '摘帽', '装配式建筑', '足球概念', '租售同权', '自由贸易港', '3D打印', '4G5G']\n"
     ]
    }
   ],
   "source": [
    "# Get notion list\n",
    "# From: http://q.10jqka.com.cn/gn/\n",
    "import re\n",
    "\n",
    "file_object = open('../data/tonghuashun_gn.txt')\n",
    "try:\n",
    "     gn_text = file_object.read( )\n",
    "finally:\n",
    "     file_object.close()\n",
    "\n",
    "gn_list = re.findall(r\">(.+?)</a>\", gn_text)\n",
    "print(gn_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 新闻分词\n",
    "\n",
    "我们使用jieba分词对每条新闻进行分词处理。\n",
    "\n",
    "输入：文本（string)\n",
    "输出：分词结果（list）\n",
    "\n",
    "后续需要进行的工作包括：\n",
    "\n",
    "* 增加财经类专有词汇进入jieba的词典\n",
    "* 增加停用词词典，以过滤无效分词。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Count\n",
    "\n",
    "# wordcount方法，输入分词结果的list，对于每个单词进行数量统计\n",
    "\n",
    "def wordcount(word_list):\n",
    "    '''\n",
    "    input: l as a list of strings\n",
    "    output: dict of {word:count}\n",
    "    '''\n",
    "    count = {}\n",
    "    for i in word_list:\n",
    "        if i in count.keys():\n",
    "            count[i] += 1\n",
    "        else:\n",
    "            count[i] = 1\n",
    "    return count\n",
    "\n",
    "# word_key_count 方法，输入分词结果list，对于特定的keywords中的词语进行统计（非keyword直接忽略）\n",
    "\n",
    "def word_key_count(wordlist,keywords):\n",
    "    '''\n",
    "    input: wordlist as a list of strings\n",
    "        keywords as a list of keywords\n",
    "    output: dict of {keyword:count}\n",
    "    '''\n",
    "    count = {}\n",
    "    for i in wordlist:\n",
    "        if i in keywords:\n",
    "            if i in count.keys():\n",
    "                count[i] += 1\n",
    "            else:\n",
    "                count[i] = 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'债转股': 5682, '电子商务': 3002, '在线教育': 1132, '生物质能': 139, '稀缺资源': 286, '集成电路': 3597, '特斯拉': 7089, '蓝宝石': 326, '禽流感': 131, '家用电器': 4422, '网约车': 194, '超导': 398, '文化传媒': 1001, '核电': 6585, '手机游戏': 102, '锂电池': 3288, '草甘膦': 370, 'OLED': 1017, '太阳能': 2185, '特钢': 6309, '迪士尼': 2185, '黄金': 17328, '农机': 1602, '人工智能': 7496, '碳纤维': 261, '白酒': 4906, '可燃冰': 227, 'PM2.5': 387, '电子竞技': 140, '网络安全': 5330, '无人驾驶': 1685, '4G5G': 6, '建筑节能': 78, '创投': 4642, '虚拟现实': 1305, '污水处理': 869, '智能家居': 1073, '无人机': 1747, '人脸识别': 591, '电子信息': 2931, '水利': 5150, '啤酒': 39109, '国产软件': 1950, '军工': 41670, '生态农业': 126, '食品安全': 661, '举牌': 8110, '煤化工': 819, '安防': 1516, '金改': 86, '高铁': 3674, '航运': 3060, '特高压': 1395, '网络游戏': 435, '乳业': 3756, '石墨电极': 170, '水泥': 16772, '风电': 4390, '钛白粉': 1107, '分散染料': 107, '摘帽': 723, '天然气': 4591, '医疗器械': 1860, '猪肉': 697, '体育产业': 1911, '生物医药': 1216, '新能源': 24052, '白马股': 3082, '高校': 1130}\n"
     ]
    }
   ],
   "source": [
    "## 对于目前的所有新闻，每条新闻进行关键词对应，查看关键词覆盖情况\n",
    "\n",
    "file = open(DATA_PATH,'r')\n",
    "\n",
    "#news_gn = pd.DataFrame(columns = [\"news_id\"] + gn_list)\n",
    "\n",
    "for (tag,line) in enumerate(file):\n",
    "    words = line.split(\" \")\n",
    "    for word in words:\n",
    "        if word in gn_list:\n",
    "            if word in gn_count.keys():\n",
    "                gn_count[word] += 1\n",
    "            else:\n",
    "                gn_count[word] = 1\n",
    "    #key_count = word_key_count(words,gn_list)\n",
    "    #print(key_count)\n",
    "    #print(\"----------\")\n",
    "print(gn_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "    require.config({\n",
       "         paths:{\n",
       "           'echarts': '/nbextensions/echarts/echarts.min'\n",
       "         }\n",
       "    });\n",
       "</script>\n",
       "<div id=\"5d853e516a734a71a10b2ae6da7690f3\" style=\"width:800px; height:400px;\"></div>\n",
       "\n",
       "<script>\n",
       "    require([ 'echarts' ],function(ec){\n",
       "\tvar myChart = ec.init(document.getElementById('5d853e516a734a71a10b2ae6da7690f3'));\n",
       "var option =  {\n",
       "    \"color\": [\n",
       "        \"#c23531\",\n",
       "        \"#2f4554\",\n",
       "        \"#61a0a8\",\n",
       "        \"#d48265\",\n",
       "        \"#749f83\",\n",
       "        \"#ca8622\",\n",
       "        \"#bda29a\",\n",
       "        \"#6e7074\",\n",
       "        \"#546570\",\n",
       "        \"#c4ccd3\",\n",
       "        \"#f05b72\",\n",
       "        \"#ef5b9c\",\n",
       "        \"#f47920\",\n",
       "        \"#905a3d\",\n",
       "        \"#fab27b\",\n",
       "        \"#2a5caa\",\n",
       "        \"#444693\",\n",
       "        \"#726930\",\n",
       "        \"#b2d235\",\n",
       "        \"#6d8346\",\n",
       "        \"#ac6767\",\n",
       "        \"#1d953f\",\n",
       "        \"#6950a1\",\n",
       "        \"#918597\",\n",
       "        \"#f6f5ec\"\n",
       "    ],\n",
       "    \"toolbox\": {\n",
       "        \"feature\": {\n",
       "            \"restore\": {\n",
       "                \"show\": true\n",
       "            },\n",
       "            \"saveAsImage\": {\n",
       "                \"title\": \"\\u4e0b\\u8f7d\\u56fe\\u7247\",\n",
       "                \"show\": true\n",
       "            },\n",
       "            \"dataView\": {\n",
       "                \"show\": true\n",
       "            }\n",
       "        },\n",
       "        \"left\": \"95%\",\n",
       "        \"top\": \"center\",\n",
       "        \"orient\": \"vertical\",\n",
       "        \"show\": true\n",
       "    },\n",
       "    \"series\": [\n",
       "        {\n",
       "            \"name\": \"precipitation\",\n",
       "            \"type\": \"bar\",\n",
       "            \"barCategoryGap\": \"20%\",\n",
       "            \"label\": {\n",
       "                \"emphasis\": {\n",
       "                    \"textStyle\": {\n",
       "                        \"color\": \"#fff\",\n",
       "                        \"fontSize\": 12\n",
       "                    },\n",
       "                    \"position\": null,\n",
       "                    \"show\": true\n",
       "                },\n",
       "                \"normal\": {\n",
       "                    \"formatter\": null,\n",
       "                    \"textStyle\": {\n",
       "                        \"color\": \"#000\",\n",
       "                        \"fontSize\": 12\n",
       "                    },\n",
       "                    \"position\": \"top\",\n",
       "                    \"show\": false\n",
       "                }\n",
       "            },\n",
       "            \"seriesId\": 2559129,\n",
       "            \"markPoint\": {\n",
       "                \"data\": [\n",
       "                    {\n",
       "                        \"name\": \"Maximum\",\n",
       "                        \"type\": \"max\",\n",
       "                        \"label\": {\n",
       "                            \"normal\": {\n",
       "                                \"textStyle\": {\n",
       "                                    \"color\": \"#fff\"\n",
       "                                }\n",
       "                            }\n",
       "                        },\n",
       "                        \"symbolSize\": 50,\n",
       "                        \"symbol\": \"pin\",\n",
       "                        \"valueDim\": null\n",
       "                    },\n",
       "                    {\n",
       "                        \"name\": \"Minimum\",\n",
       "                        \"type\": \"min\",\n",
       "                        \"label\": {\n",
       "                            \"normal\": {\n",
       "                                \"textStyle\": {\n",
       "                                    \"color\": \"#fff\"\n",
       "                                }\n",
       "                            }\n",
       "                        },\n",
       "                        \"symbolSize\": 50,\n",
       "                        \"symbol\": \"pin\",\n",
       "                        \"valueDim\": null\n",
       "                    }\n",
       "                ]\n",
       "            },\n",
       "            \"data\": [\n",
       "                3805,\n",
       "                2016,\n",
       "                756,\n",
       "                93,\n",
       "                193,\n",
       "                2412,\n",
       "                4735,\n",
       "                218,\n",
       "                89,\n",
       "                2956,\n",
       "                130,\n",
       "                266,\n",
       "                670,\n",
       "                4403,\n",
       "                69,\n",
       "                2204,\n",
       "                247,\n",
       "                682,\n",
       "                1459,\n",
       "                4213,\n",
       "                1457,\n",
       "                11642,\n",
       "                1070,\n",
       "                5078,\n",
       "                176,\n",
       "                3307,\n",
       "                152,\n",
       "                259,\n",
       "                94,\n",
       "                3566,\n",
       "                1126,\n",
       "                4,\n",
       "                52,\n",
       "                3103,\n",
       "                871,\n",
       "                580,\n",
       "                717,\n",
       "                1170,\n",
       "                397,\n",
       "                1955,\n",
       "                3438,\n",
       "                26119,\n",
       "                1305,\n",
       "                27827,\n",
       "                84,\n",
       "                443,\n",
       "                5418,\n",
       "                548,\n",
       "                1018,\n",
       "                59,\n",
       "                2466,\n",
       "                2048,\n",
       "                931,\n",
       "                294,\n",
       "                2506,\n",
       "                120,\n",
       "                11196,\n",
       "                2954,\n",
       "                739,\n",
       "                72,\n",
       "                495,\n",
       "                3090,\n",
       "                1244,\n",
       "                468,\n",
       "                1274,\n",
       "                822,\n",
       "                16121,\n",
       "                2093,\n",
       "                759\n",
       "            ],\n",
       "            \"stack\": \"\",\n",
       "            \"markLine\": {\n",
       "                \"data\": [\n",
       "                    {\n",
       "                        \"name\": \"mean-Value\",\n",
       "                        \"type\": \"average\",\n",
       "                        \"valueDim\": null\n",
       "                    }\n",
       "                ],\n",
       "                \"symbolSize\": 10\n",
       "            }\n",
       "        }\n",
       "    ],\n",
       "    \"tooltip\": {\n",
       "        \"axisPointer\": {\n",
       "            \"type\": \"line\"\n",
       "        },\n",
       "        \"textStyle\": {\n",
       "            \"color\": \"#fff\",\n",
       "            \"fontSize\": 14\n",
       "        },\n",
       "        \"formatter\": null,\n",
       "        \"triggerOn\": \"mousemove|click\",\n",
       "        \"trigger\": \"item\"\n",
       "    },\n",
       "    \"series_id\": 2559129,\n",
       "    \"backgroundColor\": \"#fff\",\n",
       "    \"yAxis\": [\n",
       "        {\n",
       "            \"nameGap\": 25,\n",
       "            \"axisLabel\": {\n",
       "                \"margin\": 8,\n",
       "                \"formatter\": \"{value} \",\n",
       "                \"textStyle\": {\n",
       "                    \"color\": \"#000\",\n",
       "                    \"fontSize\": 12\n",
       "                },\n",
       "                \"interval\": \"auto\",\n",
       "                \"rotate\": 0\n",
       "            },\n",
       "            \"nameLocation\": \"middle\",\n",
       "            \"show\": true,\n",
       "            \"name\": \"\",\n",
       "            \"type\": \"value\",\n",
       "            \"min\": null,\n",
       "            \"max\": null,\n",
       "            \"nameTextStyle\": {\n",
       "                \"fontSize\": 14\n",
       "            },\n",
       "            \"inverse\": false,\n",
       "            \"axisTick\": {\n",
       "                \"alignWithLabel\": false\n",
       "            },\n",
       "            \"position\": null,\n",
       "            \"boundaryGap\": true\n",
       "        }\n",
       "    ],\n",
       "    \"title\": [\n",
       "        {\n",
       "            \"subtext\": \"precipitation and evaporation one year\",\n",
       "            \"subtextStyle\": {\n",
       "                \"color\": \"#aaa\",\n",
       "                \"fontSize\": 12\n",
       "            },\n",
       "            \"top\": \"auto\",\n",
       "            \"text\": \"Bar chart\",\n",
       "            \"left\": \"auto\",\n",
       "            \"textStyle\": {\n",
       "                \"color\": \"#000\",\n",
       "                \"fontSize\": 18\n",
       "            }\n",
       "        }\n",
       "    ],\n",
       "    \"xAxis\": [\n",
       "        {\n",
       "            \"nameGap\": 25,\n",
       "            \"axisLabel\": {\n",
       "                \"margin\": 8,\n",
       "                \"textStyle\": {\n",
       "                    \"color\": \"#000\",\n",
       "                    \"fontSize\": 12\n",
       "                },\n",
       "                \"rotate\": 0,\n",
       "                \"interval\": \"auto\"\n",
       "            },\n",
       "            \"nameLocation\": \"middle\",\n",
       "            \"show\": true,\n",
       "            \"name\": \"\",\n",
       "            \"type\": \"category\",\n",
       "            \"min\": null,\n",
       "            \"max\": null,\n",
       "            \"nameTextStyle\": {\n",
       "                \"fontSize\": 14\n",
       "            },\n",
       "            \"inverse\": false,\n",
       "            \"data\": [\n",
       "                \"\\u503a\\u8f6c\\u80a1\",\n",
       "                \"\\u7535\\u5b50\\u5546\\u52a1\",\n",
       "                \"\\u5728\\u7ebf\\u6559\\u80b2\",\n",
       "                \"\\u751f\\u7269\\u8d28\\u80fd\",\n",
       "                \"\\u7a00\\u7f3a\\u8d44\\u6e90\",\n",
       "                \"\\u96c6\\u6210\\u7535\\u8def\",\n",
       "                \"\\u7279\\u65af\\u62c9\",\n",
       "                \"\\u84dd\\u5b9d\\u77f3\",\n",
       "                \"\\u79bd\\u6d41\\u611f\",\n",
       "                \"\\u5bb6\\u7528\\u7535\\u5668\",\n",
       "                \"\\u7f51\\u7ea6\\u8f66\",\n",
       "                \"\\u8d85\\u5bfc\",\n",
       "                \"\\u6587\\u5316\\u4f20\\u5a92\",\n",
       "                \"\\u6838\\u7535\",\n",
       "                \"\\u624b\\u673a\\u6e38\\u620f\",\n",
       "                \"\\u9502\\u7535\\u6c60\",\n",
       "                \"\\u8349\\u7518\\u81a6\",\n",
       "                \"OLED\",\n",
       "                \"\\u592a\\u9633\\u80fd\",\n",
       "                \"\\u7279\\u94a2\",\n",
       "                \"\\u8fea\\u58eb\\u5c3c\",\n",
       "                \"\\u9ec4\\u91d1\",\n",
       "                \"\\u519c\\u673a\",\n",
       "                \"\\u4eba\\u5de5\\u667a\\u80fd\",\n",
       "                \"\\u78b3\\u7ea4\\u7ef4\",\n",
       "                \"\\u767d\\u9152\",\n",
       "                \"\\u53ef\\u71c3\\u51b0\",\n",
       "                \"PM2.5\",\n",
       "                \"\\u7535\\u5b50\\u7ade\\u6280\",\n",
       "                \"\\u7f51\\u7edc\\u5b89\\u5168\",\n",
       "                \"\\u65e0\\u4eba\\u9a7e\\u9a76\",\n",
       "                \"4G5G\",\n",
       "                \"\\u5efa\\u7b51\\u8282\\u80fd\",\n",
       "                \"\\u521b\\u6295\",\n",
       "                \"\\u865a\\u62df\\u73b0\\u5b9e\",\n",
       "                \"\\u6c61\\u6c34\\u5904\\u7406\",\n",
       "                \"\\u667a\\u80fd\\u5bb6\\u5c45\",\n",
       "                \"\\u65e0\\u4eba\\u673a\",\n",
       "                \"\\u4eba\\u8138\\u8bc6\\u522b\",\n",
       "                \"\\u7535\\u5b50\\u4fe1\\u606f\",\n",
       "                \"\\u6c34\\u5229\",\n",
       "                \"\\u5564\\u9152\",\n",
       "                \"\\u56fd\\u4ea7\\u8f6f\\u4ef6\",\n",
       "                \"\\u519b\\u5de5\",\n",
       "                \"\\u751f\\u6001\\u519c\\u4e1a\",\n",
       "                \"\\u98df\\u54c1\\u5b89\\u5168\",\n",
       "                \"\\u4e3e\\u724c\",\n",
       "                \"\\u7164\\u5316\\u5de5\",\n",
       "                \"\\u5b89\\u9632\",\n",
       "                \"\\u91d1\\u6539\",\n",
       "                \"\\u9ad8\\u94c1\",\n",
       "                \"\\u822a\\u8fd0\",\n",
       "                \"\\u7279\\u9ad8\\u538b\",\n",
       "                \"\\u7f51\\u7edc\\u6e38\\u620f\",\n",
       "                \"\\u4e73\\u4e1a\",\n",
       "                \"\\u77f3\\u58a8\\u7535\\u6781\",\n",
       "                \"\\u6c34\\u6ce5\",\n",
       "                \"\\u98ce\\u7535\",\n",
       "                \"\\u949b\\u767d\\u7c89\",\n",
       "                \"\\u5206\\u6563\\u67d3\\u6599\",\n",
       "                \"\\u6458\\u5e3d\",\n",
       "                \"\\u5929\\u7136\\u6c14\",\n",
       "                \"\\u533b\\u7597\\u5668\\u68b0\",\n",
       "                \"\\u732a\\u8089\",\n",
       "                \"\\u4f53\\u80b2\\u4ea7\\u4e1a\",\n",
       "                \"\\u751f\\u7269\\u533b\\u836f\",\n",
       "                \"\\u65b0\\u80fd\\u6e90\",\n",
       "                \"\\u767d\\u9a6c\\u80a1\",\n",
       "                \"\\u9ad8\\u6821\"\n",
       "            ],\n",
       "            \"axisTick\": {\n",
       "                \"alignWithLabel\": false\n",
       "            },\n",
       "            \"position\": null,\n",
       "            \"boundaryGap\": true\n",
       "        }\n",
       "    ],\n",
       "    \"legend\": [\n",
       "        {\n",
       "            \"top\": \"top\",\n",
       "            \"data\": [\n",
       "                \"precipitation\"\n",
       "            ],\n",
       "            \"show\": true,\n",
       "            \"textStyle\": {\n",
       "                \"color\": \"#333\",\n",
       "                \"fontSize\": 12\n",
       "            },\n",
       "            \"left\": \"center\",\n",
       "            \"orient\": \"horizontal\",\n",
       "            \"selectedMode\": \"multiple\"\n",
       "        }\n",
       "    ]\n",
       "};\n",
       "myChart.setOption(option);\n",
       "\n",
       "    });\n",
       "</script>\n"
      ],
      "text/plain": [
       "<pyecharts.charts.bar.Bar at 0x7fdae775c128>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pyecharts import Bar\n",
    "\n",
    "attr = list(gn_count.keys())\n",
    "v1 = list(gn_count.values())\n",
    "bar = Bar(\"Bar chart\", \"precipitation and evaporation one year\")\n",
    "bar.add(\"precipitation\", attr, v1, mark_line=[\"average\"], mark_point=[\"max\", \"min\"])\n",
    "bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for +: 'dict_items' and 'dict_items'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-139-4612eddd98bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0md1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'集成电路'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'军工'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'人工智能'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'新能源'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'高校'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'集成电路'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdictMerged1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0md2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for +: 'dict_items' and 'dict_items'"
     ]
    }
   ],
   "source": [
    "d1 = {'集成电路': 1}\n",
    "d2 = {'军工': 1, '人工智能': 1, '新能源': 2, '高校': 1, '集成电路': 1}\n",
    "\n",
    "def "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.833 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "## jieba\n",
    "# imput：string\n",
    "# output: list of words\n",
    "\n",
    "import jieba  \n",
    "\n",
    "news_list = test_data.loc[1:10,].content.tolist()\n",
    "for news in news_list:\n",
    "    news = str(news).replace('\\t', '').replace('\\n', '').replace(' ','')\n",
    "    #seg = jieba.cut(news, cut_all=False)\n",
    "    #print((\" \".join(seg)))\n",
    "    news_jieba = ','.join(jieba.cut_for_search(news)).split(\",\")\n",
    "    #news_words = ','.join(jieba.cut_for_search(news)).split(\",\")\n",
    "    news_keywords = word_key_count(news_jieba,gn_list)\n",
    "    #print(news_keywords)\n",
    "    #print(\"-----------------\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述结果可以看到，同花顺的板块关键词与新闻中能够分词得到的关键词之间有很大的距离，很多新闻不包含板块列表中的任何一个关键词。因而，不能单纯的以同花顺板块关键词作为文章的特征向量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/inesa/.local/lib/python3.5/site-packages/gensim/models/doc2vec.py:366: UserWarning: The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\n",
      "  warnings.warn(\"The parameter `size` is deprecated, will be removed in 4.0.0, use `vector_size` instead.\")\n",
      "/home/inesa/.local/lib/python3.5/site-packages/ipykernel_launcher.py:49: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word '农业银行公告，拟以非公开发行方式发行不超过274.73亿股A股' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-437d4763702a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"农业银行公告，拟以非公开发行方式发行不超过274.73亿股A股\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1396\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1397\u001b[0m                 )\n\u001b[0;32m-> 1398\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/base_any2vec.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeyedvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mWordEmbeddingsKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m         \"\"\"\n\u001b[0;32m--> 696\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Method will be removed in 4.0.0, use self.wv.wmdistance() instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    363\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word '农业银行公告，拟以非公开发行方式发行不超过274.73亿股A股' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "import jieba  \n",
    "import sys  \n",
    "from collections import namedtuple\n",
    "from gensim.models import doc2vec\n",
    "\n",
    "\n",
    "news_contents = test_data.loc[1:10,].content.tolist()\n",
    "news_titles = test_data.loc[1:10,].title.tolist()\n",
    "# 加载停用词\n",
    "def stopwordslist(filepath):  \n",
    "    stopwords = [line.strip() for line in open(filepath, 'r', encoding='utf-8').readlines()]  \n",
    "    return stopwords  \n",
    "stop_words = stopwordslist(\"../utils/stopwords.txt\")\n",
    "\n",
    "\n",
    "def jieba_split(content):\n",
    "    '''\n",
    "    content: 输入string\n",
    "    返回：list，jieba分词结果\n",
    "    '''\n",
    "    str_content = str(content).replace('\\t', '').replace('\\n', '').replace(' ','')\n",
    "    words = ','.join(jieba.cut_for_search(news)).split(\",\")\n",
    "    ret_list = []\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            if word != '\\t':\n",
    "                ret_list.append(word)\n",
    "    return ret_list\n",
    "\n",
    "\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "# 对于读取的csv中的每行，title直接作为ID，content进行分词\n",
    "for index, row in test_data.iterrows():\n",
    "    words = jieba_split(row.content)\n",
    "    tags = [index]\n",
    "\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "    \n",
    "model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)\n",
    "\n",
    "# Get the vectors\n",
    "\n",
    "model.docvecs[0]\n",
    "model.docvecs[1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.4359948e-04,  2.4775974e-03, -3.3414126e-03,  2.0195886e-03,\n",
       "        3.1975079e-03, -2.3209287e-03, -4.8111733e-03, -1.0556693e-04,\n",
       "       -4.6941759e-03,  8.6439901e-04, -6.0576486e-04,  3.6755938e-03,\n",
       "        4.7860360e-03, -3.5068085e-03, -1.9612887e-03,  6.6922337e-04,\n",
       "        3.9420077e-03,  2.2369092e-03, -3.0673554e-03,  3.7408150e-03,\n",
       "       -1.4501261e-03,  4.6342192e-03,  2.3098721e-03, -4.4978056e-03,\n",
       "        4.6808324e-03,  4.4101542e-03, -1.8619066e-04,  3.7129037e-03,\n",
       "        1.0040315e-03,  4.2312015e-03,  1.7201813e-03,  2.8457702e-04,\n",
       "        2.2850025e-03, -7.9294865e-04, -1.8799296e-03,  3.7282121e-03,\n",
       "       -3.3937453e-03,  1.0326292e-03, -3.0222947e-03, -3.7595741e-03,\n",
       "       -2.7006185e-03, -1.3564046e-03,  3.5538755e-03, -1.8512841e-03,\n",
       "       -4.6294578e-03, -4.5664459e-03,  3.4132274e-03,  5.4600608e-04,\n",
       "        4.6899985e-03,  2.8150952e-03,  1.4626229e-03,  1.9676671e-03,\n",
       "        4.9300781e-03,  5.0489849e-05, -6.9381681e-04, -2.6175675e-03,\n",
       "       -2.8738368e-03,  1.8589342e-03, -1.6758419e-03, -4.7412124e-03,\n",
       "       -1.6757516e-03,  1.2087821e-03,  4.4598398e-03, -2.8066437e-03,\n",
       "        1.4959322e-03,  2.5511980e-03,  8.6957234e-04, -4.6138209e-03,\n",
       "       -7.3039933e-04, -3.6161735e-03,  2.4709667e-04,  1.2982028e-03,\n",
       "       -1.2230397e-04, -4.2607035e-03, -3.8072371e-03, -4.8361863e-03,\n",
       "        2.1853023e-03,  2.9918891e-03, -1.8474633e-03, -1.6187584e-03,\n",
       "       -1.8993358e-03,  1.5900822e-03, -3.1699864e-03,  1.2803908e-03,\n",
       "       -9.1701996e-04, -4.7037415e-03,  1.1838719e-03, -2.8460259e-03,\n",
       "       -2.4618064e-03,  1.5572514e-03, -1.5108566e-03, -1.1715039e-03,\n",
       "        6.7387024e-05, -2.1543100e-03,  4.9384516e-03, -3.3656592e-04,\n",
       "       -4.2337188e-03,  1.9735340e-03,  4.8039434e-03, -1.4595834e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector(['大数据'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "from gensim.models import doc2vec\n",
    "from collections import namedtuple\n",
    "\n",
    "# Load data\n",
    "\n",
    "doc1 = [\"This is a sentence\", \"This is another sentence\"]\n",
    "\n",
    "# Transform data (you can add more data preprocessing steps) \n",
    "\n",
    "docs = []\n",
    "analyzedDocument = namedtuple('AnalyzedDocument', 'words tags')\n",
    "for i, text in enumerate(doc1):\n",
    "    words = text.lower().split()\n",
    "    tags = [i]\n",
    "    docs.append(analyzedDocument(words, tags))\n",
    "\n",
    "# Train model (set min_count = 1, if you want the model to work with the provided example data set)\n",
    "\n",
    "model = doc2vec.Doc2Vec(docs, size = 100, window = 300, min_count = 1, workers = 4)\n",
    "\n",
    "# Get the vectors\n",
    "\n",
    "model.docvecs[0]\n",
    "model.docvecs[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
